{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressing Word Embeddings\n",
    "\n",
    "Downloadable version of GloVe embedding (with fallback source).\n",
    "\n",
    "Probably best to include instructions for Levy test-suite installation, so that any given embedding can be tested.\n",
    "\n",
    "Then require two main sections : \n",
    " \n",
    "*  Lloyd embedding generation\n",
    "\n",
    "*  Sparsified embedding generation\n",
    "\n",
    "Include downloadable version of sparsified GloVe embedding from own hosting.\n",
    "\n",
    "And functions/tools to play with the loaded embedding (of whatever type)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Embeddings\n",
    "\n",
    "The following needs to be Pythonized :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# http://redcatlabs.com/downloads/deep-learning-workshop/LICENSE\n",
    "RCL_BASE=http://redcatlabs.com/downloads/deep-learning-workshop/notebooks/data\n",
    "\n",
    "# Fall-back locations for the gloVe embedding\n",
    "if [ '' ] && [ ! -e \"glove.6B.300d.hkl\" ]; then\n",
    "  # Files in : ${RCL_BASE}/research/ICONIP-2016/\n",
    "  #   507.206.240 Oct 25  2015 glove.6B.300d.hkl\n",
    "  \n",
    "  # Files in : ${RCL_BASE}/research/ICONIP-2016/  :: These are originals - citation desired...\n",
    "  #    53.984.642 May 15 14:13 sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_4000_GPU-sparse_matrix.hkl\n",
    "  #   122.248.980 May  2 13:09 sparse.6B.300d_T-21_3500.1024@0.05-GPU-sparse_matrix.hkl\n",
    "  #   447.610.336 May  2 13:04 sparse.6B.300d_T-21_3500.1024@0.05-GPU-sparsity_recreate.hkl\n",
    "  #   160.569.440 May 14 14:57 vectors.2-17.hkl\n",
    "fi\n",
    "\"\"\"\n",
    "\n",
    "import os, requests\n",
    "\n",
    "def get_embedding_file( hkl ):  \n",
    "    if not os.path.isfile(os.path.join('embeddings', hkl)):\n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Download the Omer-Levy Test Regime\n",
    "\n",
    "https://levyomer.files.wordpress.com/2015/03/improving-distributional-similarity-tacl-2015.pdf\n",
    "\n",
    "```\n",
    "wget https://bitbucket.org/omerlevy/hyperwords/get/688addd64ca2.zip\n",
    "unzip 688addd64ca2.zip\n",
    "rm 688addd64ca2.zip\n",
    "\n",
    "mv omerlevy-hyperwords-688addd64ca2 omerlevy\n",
    "\n",
    "chmod 755 omerlevy/*.sh omerlevy/scripts/*.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to test a (text) Embedding\n",
    "\n",
    "Based on this script : \n",
    "```\n",
    "more omerlevy/test-vectors.sh \n",
    "#!/bin/sh\n",
    "\n",
    "# ./test-vectors.sh /home/andrewsm/sketchpad/redcatlabs/embeddings/data/1-glove-1-billion-and-wiki/window11-lc-36/vectors.txt \n",
    "\n",
    "# arg1 == filepath of word-vectors file\n",
    "VECTORS_FILE=$1\n",
    "  \n",
    "# Fix up the 'file header' of a 'glove' vectors file into the one expected here\n",
    "VECTORS_WORDS=${VECTORS_FILE}.words\n",
    "\n",
    "if [ ! -f ${VECTORS_WORDS} ]; then \n",
    "  echo \"Creating ${VECTORS_WORDS}\"\n",
    "  #echo \"262144 300\" > ${VECTORS_WORDS}\n",
    "  #head -262144 ${VECTORS_FILE} >> ${VECTORS_WORDS}\n",
    "\n",
    "  ## Glove min-freq : 36 -> 263633 words (just above 12^18=262144 words)\n",
    "  echo \"131072 300\" > ${VECTORS_WORDS}\n",
    "  head -131072 ${VECTORS_FILE} >> ${VECTORS_WORDS}\n",
    "fi\n",
    "\n",
    "VECTORS_NPY=${VECTORS_WORDS}.npy\n",
    "\n",
    "\n",
    "#word2vecf/word2vecf -train w2.sub/pairs -pow 0.75 -cvocab w2.sub/counts.contexts.vocab -wvocab w2.sub/counts.words.vocab -dumpcv w2.sub/sgns.contexts -output w2.sub/sgns.words -threads 10 -\n",
    "negative 15 -size 500;\n",
    "\n",
    "python hyperwords/text2numpy.py ${VECTORS_WORDS}\n",
    "\n",
    "# No need for this temporary file now\n",
    "##rm ${VECTORS_WORDS}\n",
    "\n",
    "\n",
    "#python hyperwords/text2numpy.py w2.sub/sgns.contexts\n",
    "#rm w2.sub/sgns.contexts\n",
    "\n",
    "\n",
    "echo\n",
    "echo \"Similarity\"\n",
    "echo \"----------\"\n",
    "# Evaluate on Word Similarity\n",
    "#python hyperwords/ws_eval.py --neg 5 PPMI  w2.sub/pmi testsets/ws/ws353.txt\n",
    "#python hyperwords/ws_eval.py --eig 0.5 SVD w2.sub/svd testsets/ws/ws353.txt\n",
    "#python hyperwords/ws_eval.py --w+c SGNS    w2.sub/sgns testsets/ws/ws353.txt\n",
    "\n",
    "#echo -n \"WS353 Results     \"\n",
    "#python hyperwords/ws_eval.py VECTORS ${VECTORS_FILE} testsets/ws/ws353.txt\n",
    "\n",
    "echo -n \"WS353 Similarity  \"\n",
    "python hyperwords/ws_eval.py VECTORS ${VECTORS_FILE} testsets/ws/ws353_similarity.txt\n",
    "\n",
    "echo -n \"WS353 Relatedness \"\n",
    "python hyperwords/ws_eval.py VECTORS ${VECTORS_FILE} testsets/ws/ws353_relatedness.txt\n",
    "\n",
    "echo -n \"Bruni MEN         \"\n",
    "python hyperwords/ws_eval.py VECTORS ${VECTORS_FILE} testsets/ws/bruni_men.txt\n",
    "\n",
    "echo -n \"Radinsky M.Turk   \"\n",
    "python hyperwords/ws_eval.py VECTORS ${VECTORS_FILE} testsets/ws/radinsky_mturk.txt\n",
    "\n",
    "echo -n \"Luoung Rare Words \"\n",
    "python hyperwords/ws_eval.py VECTORS ${VECTORS_FILE} testsets/ws/luong_rare.txt\n",
    "\n",
    "echo\n",
    "echo \"Geometry\"\n",
    "echo \"--------\"\n",
    "# Evaluate on Analogies\n",
    "#python hyperwords/analogy_eval.py PPMI        w2.sub/pmi testsets/analogy/google.txt\n",
    "#python hyperwords/analogy_eval.py --eig 0 SVD w2.sub/svd testsets/analogy/google.txt\n",
    "#python hyperwords/analogy_eval.py SGNS        w2.sub/sgns testsets/analogy/google.txt\n",
    "\n",
    "echo -n \"Google Analogy Results  \"\n",
    "python hyperwords/analogy_eval.py VECTORS ${VECTORS_FILE} testsets/analogy/google.txt\n",
    "\n",
    "echo -n \"MSR Analogy Results     \"\n",
    "python hyperwords/analogy_eval.py VECTORS ${VECTORS_FILE} testsets/analogy/msr.txt\n",
    "\n",
    "echo\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "\n",
    "def test_embedding_file(vectors_txt, vocab_max=131072 ):\n",
    "    # Do we need to process VECTORS_FILE->{ VECTORS_WORDS, VECTORS_NPY }?\n",
    "    # Answer = YES : the .words is required, and is used to create .npy and .vocab\n",
    "    \n",
    "    vectors_txt_words = '%s.words' % (vectors_txt,)\n",
    "    if not os.path.isfile(vectors_txt_words):\n",
    "        # This is just a copy of 'text file' with the vocab_size and embedding_size pre-pended\n",
    "        #echo \"131072 300\" > ${VECTORS_WORDS}\n",
    "        #head -131072 ${VECTORS_FILE} >> ${VECTORS_WORDS}\n",
    "        with open(vectors_txt) as fin:\n",
    "            first_line = fim.readline()\n",
    "            embedding_dim = len(first_line.strip().split()) -1 \n",
    "            vocab_size = len(fin.readlines()) +1  # Ouch! - read in whole file to find length\n",
    "\n",
    "        if vocab_size>vocab_max:\n",
    "            vocab_size=vocab_max\n",
    "            \n",
    "        with open(vectors_txt) as fin:\n",
    "            with open(vectors_txt_words, 'wt') as fout:\n",
    "                # Write the first line, which, ironically, will be discarded by the omerlevy code\n",
    "                fout.write(\"%d %d\\n\" % (vocab_size, embedding_dim))\n",
    "                \n",
    "                # And copy over at most vocab_max lines of the original file \n",
    "                for i, line in enumerate(fin.readlines()):\n",
    "                    if i>vocab_size:\n",
    "                        break\n",
    "                    fout.write(line)\n",
    "    \n",
    "    return # Early return during download testing\n",
    "\n",
    "    vectors_txt_npy   = '%s.npy' % (vectors_txt_words,)\n",
    "    if not os.path.isfile(vectors_txt_words):\n",
    "        # Sadly, we can't just invoke this as a python function - need to go via shell...\n",
    "        subprocess.call([ \"python\", \"hyperwords/text2numpy.py\", vectors_txt_words ])\n",
    "    pass\n",
    "    \n",
    "    def run_word_similarity(test_str, test_set):\n",
    "        print(test_str, end=None)\n",
    "        subprocess.call([ \"python\", \"hyperwords/ws_eval.py\", \"VECTORS\", vectors_txt_npy, \"testsets/ws/%s\" % (test_set,) ])\n",
    "\n",
    "    run_word_similarity(\"WS353 Similarity  \", \"ws353_similarity.txt\")\n",
    "    run_word_similarity(\"WS353 Relatedness \", \"ws353_relatedness.txt\")\n",
    "    run_word_similarity(\"Bruni MEN         \", \"bruni_men.txt\")\n",
    "    run_word_similarity(\"Radinsky M.Turk   \", \"radinsky_mturk.txt\")\n",
    "    run_word_similarity(\"Luoung Rare Words \", \"luong_rare.txt\")\n",
    "\n",
    "    #echo -n \"Google Analogy Results  \"\n",
    "    #python hyperwords/analogy_eval.py VECTORS ${VECTORS_FILE} testsets/analogy/google.txt\n",
    "\n",
    "    # Same for word_analogy() once the word_similarity is proven\n",
    "    \n",
    "    #echo -n \"MSR Analogy Results     \"\n",
    "    #python hyperwords/analogy_eval.py VECTORS ${VECTORS_FILE} testsets/analogy/msr.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lloyd's Method : 32->3 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "import argparse\n",
    "import progressbar\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "\n",
    "# http://blog.mdda.net/oss/2016/04/07/nvidia-on-fedora-23\n",
    "#theano.config.nvcc.flags = '-D_GLIBCXX_USE_CXX11_ABI=0'\n",
    "\n",
    "import scipy.spatial.distance\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import hickle\n",
    "\n",
    "default_embedding_file = '../../data/2-pretrained-vectors/glove.6B.300d.hkl'\n",
    "\n",
    "#... args ...\n",
    "\n",
    "## http://www-nlp.stanford.edu/projects/glove/?place=topic%2Fglobalvectors%2FBiiXED8vVQg%2Fdiscussion\n",
    "#While it's of course possible to convert the text files to a binary format, \n",
    "#the result would not be equivalent to the binary output from GloVe. \n",
    "#The reason is that the pre-trained vectors only contain W + \\tilde{W}, \n",
    "#i.e. the word vectors plus the context word vectors, and omit the bias terms. \n",
    "#As such you don't want to use the evaluation script without modification. \n",
    "\n",
    "d = hickle.load(args.embedding)\n",
    "vocab, embedding = d['vocab'], d['embedding']\n",
    "\n",
    "#dictionary = dict( (word.lower(), i) for i,word in enumerate(vocab) )\n",
    "#dictionary = dict( (word, i) for i,word in enumerate(vocab) )\n",
    "dictionary = dict( (word, i) for i,word in enumerate(vocab) if i<len(embedding) )\n",
    "\n",
    "print(\"Embedding loaded :\", embedding.shape)   # (vocab_size, embedding_dimension)=(rows, columns)\n",
    "\n",
    "def np_int_list(n, mult=100., size=3):  # size includes the +/-\n",
    "  #print( (n * mult).astype(int).tolist() )\n",
    "  return \"[ \" + (', '.join([ ('% +*d') % (size,x,) for x in (n * mult).astype(int).tolist()])) + \" ]\"\n",
    "\n",
    "if args.mangle is not None:\n",
    "  if args.mangle=='lloyd':    # Quantise each entry into 'pct' (as an integer) level (optimised per vector location)\n",
    "    # Suppose that v is a vector of levels\n",
    "    #          and c is a list of numbers that needs to be quantised, \n",
    "    #          each c becomes c' where c' is the closest value in v\n",
    "    #          :: update v so that (c - c')^2 is as low as possible\n",
    "\n",
    "    levels_base = int(args.pct)\n",
    "\n",
    "    c_length = embedding.shape[0]\n",
    "    \n",
    "    t0 = time.time()\n",
    "    for d in range(embedding.shape[1]):   # One-dimensional version\n",
    "      levels = levels_base  \n",
    "\n",
    "      \"\"\"\n",
    "  9 - took 223\n",
    " 78 - took 222\n",
    "121 - took 214\n",
    "135 - took 203\n",
    "150 - took 237\n",
    "186 - took 206\n",
    "187 - took 205\n",
    "226 - took 283\n",
    "236 - took 219\n",
    "242 - took 221\n",
    "278 - took 212\n",
    "279 - took 239\n",
    "290 - took 216\n",
    "      \"\"\"\n",
    "\n",
    "      #if d in (9, 78, 121, 135, 150, 186, 187, 226, 236, 242, 278, 279, 290):\n",
    "      #  levels = 16\n",
    "\n",
    "      i_step = int(c_length/levels)\n",
    "      i_start = int(i_step/2)\n",
    "    \n",
    "      v_indices = np.arange(start=i_start, stop=c_length, step=i_step, dtype='int')\n",
    "\n",
    "      #if d != 9: continue  # Wierd distribution\n",
    "      #if d != 1: continue  # Very standard example\n",
    "      \n",
    "\n",
    "      # Initialise v by sorting c, and placing them evenly through the list\n",
    "      \n",
    "      e_column = embedding[:,d].astype('float32')\n",
    "      \n",
    "      c_sorted = np.sort( e_column )\n",
    "      v_init = c_sorted[ v_indices ]\n",
    "\n",
    "      #zeros = np.zeros_like(c_sorted)\n",
    "\n",
    "      # the v_init are the initial centers \n",
    "      v=v_init\n",
    "      \n",
    "      t1 = time.time()\n",
    "      epochs=0\n",
    "      for epoch in range(0, 1000):\n",
    "        #err, = iterate_v(lr)\n",
    "        \n",
    "        #print(\" Dimension:%3d, Epoch:%3d, %s\" % (d, epoch, np_int_list(v),))\n",
    "        \n",
    "        #   works out the values in their middles\n",
    "        mids_np = (v[:-1] + v[1:])/2.\n",
    "        \n",
    "        mids = mids_np.tolist()\n",
    "        mids.insert( 0, c_sorted[0] )\n",
    "        mids.append( c_sorted[-1] +1 )\n",
    "        \n",
    "        centroids=[]\n",
    "        for i in range( 0, len(mids)-1 ):\n",
    "          pattern = np.where( (mids[i] <= c_sorted) & (c_sorted < mids[i+1]) )\n",
    "          centroids.append( c_sorted[ pattern ].mean() )\n",
    "        \n",
    "        centroids_np = np.array(centroids)\n",
    "\n",
    "        if np.allclose(v, centroids_np):\n",
    "          if epochs>200:\n",
    "            print(\"%3d - took %d\" % (d, epochs,))\n",
    "          break\n",
    "        \n",
    "        v = centroids_np\n",
    "        \n",
    "        epochs += 1\n",
    "        \n",
    "        \n",
    "      #print(\"Time per calc %6.2fms\" % ((time.time() - t1)/epochs*1000.,))\n",
    "      \n",
    "      #print(\"Check col updated: before \", np_int_list(embedding[0:20,d]))\n",
    "\n",
    "      # Ok, so now we have the centers in v, and the mids in 'mids'\n",
    "      for i in range( 0, len(mids)-1 ):\n",
    "        pattern = np.where( (mids[i] <= e_column) & (e_column < mids[i+1]) )\n",
    "        embedding[pattern, d] = v[i]\n",
    "\n",
    "      #print(\"Check col updated: after  \", np_int_list(embedding[0:20,d]))\n",
    "      \n",
    "    offset=101010  # Check rare-ish words\n",
    "    for d in range(5, embedding.shape[1], 25):\n",
    "      print(\"Col %3d updated: \" % (d,), np_int_list(embedding[(offset+0):(offset+20),d]))\n",
    "\n",
    "#sklearn.preprocessing.normalize(embedding, norm='l2', axis=1, copy=False)   # This is in-place\n",
    "embedding_normed = sklearn.preprocessing.normalize(embedding, norm='l2', axis=1, copy=True) \n",
    "\n",
    "\n",
    "\n",
    "if args.save:\n",
    "  # Save the embedding_normed as a text file\n",
    "  with open(args.save, 'wb') as f:\n",
    "    embedding_save = embedding_normed\n",
    "    \n",
    "    for l in range(0, embedding_save.shape[0]):\n",
    "      f.write(\"%s %s\\n\" % (vocab[l], ' '.join([ ('0' if x==0. else (\"%.6f\" % (x,))) for x in embedding_save[l, :].tolist() ]), ))\n",
    "  print(\"Saved to %s\" % (args.save, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Negative Sparse Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=\"\"\"\n",
    "\n",
    "python sparsify_lasagne.py --mode=train   --version=21 --save='./sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_%04d.hkl' --sparsity=0.0675 --random=1 --iters=4000 | tee sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75.log\n",
    "  #sparse_dim = 1024, pre-num_units=embedding_dim*8,   \n",
    "# -> 4.0 l2 in 4.0k epochs (sigma=39)  # sparsity_std_:,   0.4742,\n",
    "python sparsify_lasagne.py --mode=predict --version=21 --load='./sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_4000.hkl' --sparsity=0.0675 --random=1 \\\n",
    "      --output=sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_4000_GPU-sparsity_recreate.hkl \\\n",
    "      --direct=sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_4000_GPU-sparse_matrix.hkl \n",
    "  # epoch:4009, b:      0, l2:      3.8344, sparsity:6.7476 - hard01\n",
    "  # epoch:4009, b:  16384, l2:      3.8593, sparsity:6.7503 - hard01\n",
    "  # epoch:4009, b:  32768, l2:      3.8925, sparsity:6.7489 - hard01\n",
    "  # epoch:4009, b:  49152, l2:      3.7866, sparsity:6.7482 - hard01\n",
    "  # epoch:4009, b:  65536, l2:      3.8729, sparsity:6.7476 - hard01\n",
    "  # epoch:4009, b:  81920, l2:      3.8502, sparsity:6.7489 - hard01\n",
    "  # epoch:4009, b:  98304, l2:      3.8340, sparsity:6.7480 - hard01\n",
    "  # epoch:4009, b: 114688, l2:      3.8588, sparsity:6.7476 - hard01\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "\n",
    "import argparse\n",
    "import progressbar\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import lasagne\n",
    "\n",
    "import hickle\n",
    "\n",
    "#default_embedding_file = '../../data/2-pretrained-vectors/glove.6B.300d.hkl'\n",
    "default_embedding_file = '../../data/1-glove-1-billion-and-wiki/window11-lc-36/vectors.2-17.hkl'\n",
    "default_version=21\n",
    "default_save_file_fmt  = './sparse.6B.300d_%d_%%04d.hkl' % (default_version, )\n",
    "\n",
    "#theano.config.nvcc.flags='-D_GLIBCXX_USE_CXX11_ABI=0' # Now in .theanorc\n",
    "\n",
    "parser = argparse.ArgumentParser(description='')\n",
    "parser.add_argument('-m','--mode', help='(train|predict)', type=str, default=None)\n",
    "parser.add_argument('-v','--version', help='Model version to run', type=int, default=default_version)\n",
    "\n",
    "parser.add_argument('-i','--iters', help='Number of iterations', type=int, default=10000)\n",
    "parser.add_argument('-e','--embedding', help='Filepath of hickle file containing embedding for testing', type=str, default=default_embedding_file)\n",
    "\n",
    "parser.add_argument('-s','--save', help='Format of save filenames (use %d for epoch)', type=str, default=default_save_file_fmt)\n",
    "parser.add_argument('-l','--load', help='Load filename', type=str, default=None)\n",
    "\n",
    "parser.add_argument('-o','--output', help='Filepath of hickle file to *create* embedding for testing', type=str, default=None)\n",
    "parser.add_argument('-d','--direct', help='Filepath of hickle file to *create* *binary* embedding for testing', type=str, default=None)\n",
    "\n",
    "parser.add_argument('-p','--param', help='Set param value initially', type=float, default=None)\n",
    "parser.add_argument('-k','--sparsity',  help='Sparsity value goal', type=float, default=0.05)\n",
    "\n",
    "#parser.add_argument('-b','--batchsize', help='batchsize (GTX760 requires <20000)', type=int, default=10000)\n",
    "parser.add_argument('-b','--batchsize', help='batchsize (GTX760 requires <20000)', type=int, default=16384*1)\n",
    "\n",
    "#parser.add_argument('-t','--test', help='Which Test to execute', type=str, default=default_test)\n",
    "#parser.add_argument('-p','--pct', help='filter parameter ~ percentage (0,100)', type=float, default=None)\n",
    "parser.add_argument('-r','--random', help='Randomly shuffle vocab', type=bool, default=False)\n",
    "parser.add_argument('-n','--normalize', help='normalize embedding before learning', type=bool, default=False)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(\"Mode : %s\" % (args.mode,)) \n",
    "\n",
    "print(\"Loading embedding : %s\" % (args.embedding,)) \n",
    "\n",
    "d = hickle.load(args.embedding)\n",
    "vocab, embedding = d['vocab'], d['embedding']\n",
    "vocab_np = np.array(vocab, dtype=str)\n",
    "vocab_orig=vocab_np.copy()\n",
    "\n",
    "if args.random:\n",
    "   np.random.seed(1) # No need to get fancy - just want to mix up the word frequencies into different batches\n",
    "   perm = np.random.permutation(len(embedding))\n",
    "   embedding = embedding[perm]\n",
    "   vocab = vocab_np[perm].tolist()\n",
    "  \n",
    "dictionary = dict( (word, i) for i,word in enumerate(vocab) )\n",
    "\n",
    "print(\"Embedding loaded :\", embedding.shape)   # (vocab_size, embedding_dimension)=(rows, columns)\n",
    "\n",
    "print(\"Device=%s, OpenMP=%s\" % (theano.config.device, (\"True\" if theano.config.openmp else \"False\"), ))\n",
    "\n",
    "def np_int_list(n, mult=100., size=3):  # size includes the +/-\n",
    "  return \"[ \" + (', '.join([ ('% +*d') % (size,x,) for x in (n * mult).astype(int).tolist()])) + \" ]\"\n",
    "\n",
    "#embedding = np.copy(embedding[ 0:50000, : ])\n",
    "#embedding = embedding[ 0:50000, : ]\n",
    "#embedding = embedding[ 0:10000, : ]   # REVERT\n",
    "\n",
    "embedding_dim = embedding.shape[1]\n",
    "sparse_dim = 1024\n",
    "#sparse_dim = 1024/2\n",
    "#sparse_dim = 1024*4\n",
    "\n",
    "#batchsize = 10000   # 0.48ms GPU\n",
    "#batchsize = 20000   # 0.48ms GPU\n",
    "batchsize = args.batchsize\n",
    "\n",
    "version=args.version\n",
    "\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SparseWinnerTakeAllLayer(lasagne.layers.Layer):\n",
    "    def __init__(self, incoming, sparsity=0.05, **kwargs):\n",
    "        super(SparseWinnerTakeAllLayer, self).__init__(incoming, **kwargs)\n",
    "        self.sparsity = sparsity\n",
    "\n",
    "    def get_output_for(self, input, deterministic=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : tensor\n",
    "            output from the previous layer\n",
    "        deterministic : bool\n",
    "            If true, just use the raw values (don't sparsify)\n",
    "        \"\"\"\n",
    "        if False and deterministic:\n",
    "            return input\n",
    "            #return theano.tensor.switch( theano.tensor.gt(input, 0.), 1.0, 0.0)\n",
    "            \n",
    "        else:\n",
    "            # use nonsymbolic shape for this if possible\n",
    "            \n",
    "            #input_shape = self.input_shape\n",
    "            #if any(s is None for s in input_shape):\n",
    "            #    input_shape = input.shape            \n",
    "\n",
    "            # Sort within batch\n",
    "            # input_shape is [ #in_batch, #vector_entries ] ~ [ 20k, 1024 ]\n",
    "\n",
    "            # theano.tensor.sort(self, axis, kind, order)\n",
    "            sort_input = input.sort( axis=0, kind='quicksort' )\n",
    "            \n",
    "            # Find kth value\n",
    "            \n",
    "            hurdles_raw = sort_input[ int( batchsize * (1.0 - self.sparsity) ), : ]\n",
    "            \n",
    "            hurdles = theano.tensor.maximum(hurdles_raw, 0.0)  # rectification...\n",
    "            \n",
    "            # switch based on >kth value (or create mask)\n",
    "            # all other entries are zero\n",
    "            \n",
    "            #mask = theano.tensor.switch( theano.tensor.gt(input, hurdles), 1.0, 0.0)\n",
    "            \n",
    "            # pass those entries along verbatim\n",
    "            #return mask * input\n",
    "            \n",
    "            masked = theano.tensor.switch( theano.tensor.ge(input, hurdles), input, 0.0)\n",
    "            return masked\n",
    "\n",
    "        \n",
    "        \n",
    "class SparseWinnerTakeAllLayerApprox(lasagne.layers.Layer):\n",
    "    def __init__(self, incoming, approx_sparsity=0.12, **kwargs):  \n",
    "        super(SparseWinnerTakeAllLayerApprox, self).__init__(incoming, **kwargs)\n",
    "        self.sparsity = approx_sparsity\n",
    "\n",
    "    def get_output_for(self, input, deterministic=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : tensor\n",
    "            output from the previous layer\n",
    "        deterministic : bool\n",
    "            If true, just use the raw values (don't sparsify)\n",
    "        \"\"\"\n",
    "        # input_shape is [ #in_batch, #vector_entries ] ~ [ 20k, 1024 ]\n",
    "    \n",
    "        current_sparsity = self.sparsity\n",
    "        #print(current_sparsity)  # A theano variable\n",
    "        \n",
    "        if False:\n",
    "          # Find the max value in each column - this is the k=1 (top-most) entry\n",
    "          hurdles_max  = input.max( axis=0 )\n",
    "          \n",
    "          input = lasagne.layers.get_output(embedding_batch_middle)\n",
    "          \n",
    "          # Find the max value in each column - this is the k=1 (top-most) entry\n",
    "          hurdles_max  = input.max( axis=0 )\n",
    "          \n",
    "          # Find the min value in each column - this is the k=all (bottom-most) entry\n",
    "          #hurdles_min  = input.min( axis=0 )\n",
    "\n",
    "          # Let's guess (poorly) that the sparsity hurdle is (0... sparsity ...100%) within these bounds\n",
    "          #hurdles_guess = hurdles_max * (1.0 - current_sparsity) + hurdles_min * current_sparsity\n",
    "          \n",
    "          #hurdles_guess = (hurdles_min + hurdles_max)/2.0\n",
    "          \n",
    "          # New approach : We know that the mean() is zero and the std() is 1\n",
    "          #   simulations suggest that the more stable indicators are at fractions of the max()\n",
    "          \n",
    "          hurdles_hi = hurdles_max * 0.5\n",
    "          hurdles_lo = hurdles_max * 0.3\n",
    "          \n",
    "          # Now, let's find the actual sparsity that this creates\n",
    "          sparsity_flag_hi = theano.tensor.switch( theano.tensor.ge(input, hurdles_hi), 1.0, 0.0)\n",
    "          sparsity_real_hi = sparsity_flag_hi.mean(axis=0)    # Should be ~ sparsity (likely to be lower, though)\n",
    "\n",
    "          sparsity_flag_lo = theano.tensor.switch( theano.tensor.ge(input, hurdles_lo), 1.0, 0.0)\n",
    "          sparsity_real_lo = sparsity_flag_lo.mean(axis=0)    # Should be ~ sparsity (likely to be higher, though)\n",
    "          \n",
    "          # But this is wrong!  Let's do another estimate (will be much closer, hopefully) using this knowledge\n",
    "          #   For each column, the new hurdle guess\n",
    "          \n",
    "          #hurdles_better = hurdles_max - ( current_sparsity / (sparsity_guess_real + 0.00001) ) * (hurdles_max - hurdles_guess)\n",
    "          \n",
    "\n",
    "          if False: # This assumes that the distribution tails are linear (which is not true)\n",
    "            hurdles_interp = hurdles_hi + (hurdles_lo-hurdles_hi) * (current_sparsity - sparsity_real_hi) / ((sparsity_real_lo - sparsity_real_hi)+0.00001)\n",
    "            \n",
    "          else:  # Assume that the areas under the tails are ~ exp(-x*x)  \n",
    "            # See (2) in : https://math.uc.edu/~brycw/preprint/z-tail/z-tail.pdf\n",
    "            # *** See (Remark 15) in : http://m-hikari.com/ams/ams-2014/ams-85-88-2014/epureAMS85-88-2014.pdf\n",
    "            \n",
    "            def tail_transform(z):\n",
    "              return theano.tensor.sqrt( -theano.tensor.log( z ) )\n",
    "            \n",
    "            tail_target = tail_transform(current_sparsity)\n",
    "            tail_hi = tail_transform(sparsity_real_hi)\n",
    "            tail_lo = tail_transform(sparsity_real_lo)\n",
    "\n",
    "            hurdles_interp = hurdles_hi + (hurdles_lo-hurdles_hi) * (tail_target - tail_hi) / ((tail_lo - tail_hi)+0.00001)\n",
    "\n",
    "          \n",
    "          #hurdles = theano.tensor.maximum(hurdles_better, 0.0)  # rectification... at mininim... (also solves everything-blowing-up problem)\n",
    "          hurdles = hurdles_interp.clip(hurdles_max*0.2, hurdles_max*0.9)\n",
    "\n",
    "\n",
    "        if True:\n",
    "          hurdles_hi, hurdles_lo = [], []\n",
    "          \n",
    "          hurdles_guess = []\n",
    "          sparsity_flag = []\n",
    "          sparsity_real = []\n",
    "          \n",
    "          sparsity_hi, sparsity_lo = [], []\n",
    "\n",
    "\n",
    "          # Find the max value in each column - this is the k=1 (top-most) entry\n",
    "          hurdles_max  = input.max( axis=0 )\n",
    "          \n",
    "          hurdles_hi.append(hurdles_max)\n",
    "          sparsity_hi.append(hurdles_max * (1./batchsize) ) \n",
    "          \n",
    "\n",
    "          hurdles_lo_temp = input.mean( axis=0 )  # Different estimate idea...\n",
    "\n",
    "          hurdles_lo.append(hurdles_lo_temp)\n",
    "          sparsity_lo_temp = theano.tensor.switch( theano.tensor.ge(input, hurdles_lo_temp), 1.0, 0.0)\n",
    "          sparsity_lo.append( sparsity_lo_temp.mean(axis=0) )\n",
    "          \n",
    "          for i in range(10):  \n",
    "            if True:   # WINS THE DAY!\n",
    "              hurdles_guess.append(\n",
    "                (\n",
    "                  (hurdles_lo[-1] + hurdles_hi[-1]) * 0.5\n",
    "                )\n",
    "              )\n",
    "\n",
    "            if False:\n",
    "              hurdles_guess.append(\n",
    "                (\n",
    "                  hurdles_hi[-1] + (hurdles_lo[-1] - hurdles_hi[-1]) * \n",
    "                    (current_sparsity - sparsity_hi[-1]) / ((sparsity_lo[-1] - sparsity_hi[-1])+0.000001)\n",
    "                ).clip(hurdles_lo[-1], hurdles_hi[-1])\n",
    "              )\n",
    "\n",
    "            if False:\n",
    "              # switch on closeness to getting it correct\n",
    "              hurdles_guess.append(\n",
    "                theano.tensor.switch( theano.tensor.lt( sparsity_lo[-1], current_sparsity * 2.0 ),\n",
    "                  (\n",
    "                    hurdles_hi[-1] + (hurdles_lo[-1] - hurdles_hi[-1]) * \n",
    "                      (current_sparsity - sparsity_hi[-1]) / ((sparsity_lo[-1] - sparsity_hi[-1])+0.000001)\n",
    "                  ).clip(hurdles_lo[-1], hurdles_hi[-1]),\n",
    "                  (\n",
    "                    (hurdles_lo[-1] + hurdles_hi[-1]) * 0.5\n",
    "                  )\n",
    "                )\n",
    "                \n",
    "              )\n",
    "              \n",
    "            \n",
    "            sparsity_flag.append( theano.tensor.switch( theano.tensor.ge(input, hurdles_guess[-1] ), 1.0, 0.0) )\n",
    "            sparsity_real.append( sparsity_flag[-1].mean(axis=0) )\n",
    "            \n",
    "            # So, based on whether the real sparsity is greater or less than the real value, change the hi or lo values\n",
    "\n",
    "            hurdles_lo.append( \n",
    "              theano.tensor.switch( theano.tensor.gt(current_sparsity, sparsity_real[-1]), hurdles_lo[-1], hurdles_guess[-1]) \n",
    "            )\n",
    "            hurdles_hi.append( \n",
    "              theano.tensor.switch( theano.tensor.le(current_sparsity, sparsity_real[-1]), hurdles_hi[-1], hurdles_guess[-1]) \n",
    "            )\n",
    "\n",
    "          hurdles = hurdles_guess[-1]\n",
    "          #hurdles = hurdles_lo[-1]  # Better to bound this at the highest relevant sparsity...\n",
    "          \n",
    "        masked = theano.tensor.switch( theano.tensor.ge(input, hurdles), input, 0.0)\n",
    "        return masked\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "embedding_N = (embedding)  # No Normalization by default\n",
    "\n",
    "if args.normalize:\n",
    "  #>>> a=np.array( [ [1,-1,1,-1], [-5,5,5,-5] ])\n",
    "  #>>> b=np.std(a, axis=1)\n",
    "  #>>> a / b[:, np.newaxis]\n",
    "  #array([[ 1., -1.,  1., -1.],\n",
    "  #       [-1.,  1.,  1., -1.]])\n",
    "  \n",
    "  embedding_std  = np.std(embedding, axis=1)\n",
    "  embedding_N = embedding / embedding_std[:, np.newaxis]    # Try Normalizing  std(row) == 1, making sure shapes are right\n",
    "\n",
    "\n",
    "embedding_shared = theano.shared(embedding_N.astype('float32'))       # 400000, 300\n",
    "embedding_shared.name = \"embedding_shared\"\n",
    "\n",
    "batch_start_index = theano.tensor.scalar('batch_start_index', dtype='int32')\n",
    "\n",
    "embedding_batch = embedding_shared[ batch_start_index:(batch_start_index+batchsize) ]\n",
    "\n",
    "network = lasagne.layers.InputLayer( \n",
    "    ( batchsize, embedding_dim ), \n",
    "    input_var=embedding_batch,\n",
    "  )\n",
    "\n",
    "pre_hidden_dim=embedding_dim*8  ## For sparse_dim=1024 and below\n",
    "if sparse_dim>1024*1.5:\n",
    "  pre_hidden_dim=sparse_dim*2   ## Larger sparse_dim\n",
    "\n",
    "network = lasagne.layers.DenseLayer(\n",
    "    network,\n",
    "    num_units=pre_hidden_dim,     \n",
    "    nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    W=lasagne.init.GlorotUniform(),\n",
    "    b=lasagne.init.Constant(0.)\n",
    "  )\n",
    "\n",
    "if version==22:\n",
    "  network = lasagne.layers.DenseLayer(\n",
    "      network,\n",
    "      num_units=sparse_dim*2,\n",
    "      nonlinearity=lasagne.nonlinearities.rectify,\n",
    "      W=lasagne.init.GlorotUniform(),\n",
    "      b=lasagne.init.Constant(0.)\n",
    "    )\n",
    "\n",
    "network = lasagne.layers.DenseLayer(\n",
    "    network,\n",
    "    num_units=sparse_dim,\n",
    "    nonlinearity=lasagne.nonlinearities.identity,\n",
    "    W=lasagne.init.GlorotUniform(),\n",
    "    b=lasagne.init.Constant(0.)\n",
    "  )\n",
    "\n",
    "sparse_embedding_batch_linear=network\n",
    "\n",
    "def hard01(x):\n",
    "  # http://deeplearning.net/software/theano/library/tensor/basic.html#theano.tensor.switch\n",
    "  #return theano.tensor.switch( theano.tensor.gt(x, 0.), 0.95, 0.05)\n",
    "  return theano.tensor.switch( theano.tensor.gt(x, 0.), 1.0, 0.0)\n",
    "  \n",
    "sparse_embedding_batch_probs=None\n",
    "if args.mode == 'train':\n",
    "  sigma = theano.tensor.scalar(name='sigma', dtype='float32')\n",
    "\n",
    "  if version==21 or version==22: # winner-take-all idea (GPU-able?)\n",
    "\n",
    "    if True:\n",
    "      embedding_batch_middle = lasagne.layers.batch_norm(\n",
    "          lasagne.layers.NonlinearityLayer( network,  nonlinearity=lasagne.nonlinearities.rectify )\n",
    "          #lasagne.layers.NonlinearityLayer( network,  nonlinearity=lasagne.nonlinearities.identity ) \n",
    "        )\n",
    "\n",
    "    if True:\n",
    "      #was in for '1' and '0.1' versions\n",
    "      embedding_batch_middle = lasagne.layers.GaussianNoiseLayer(\n",
    "                embedding_batch_middle, \n",
    "                #sigma=0.1)\n",
    "                #sigma=0.1 * theano.tensor.exp((-0.1) * sigma ))  # Noise should die down over time...  (idea, slowish)  BASE\n",
    "                sigma=0.2 * theano.tensor.exp((-0.01) * sigma ))  # Noise should die down over time...  (idea, slowish)  _.2.01_\n",
    "\n",
    "\n",
    "    sparsity_blend = theano.tensor.exp((-10.) * sigma )  # Goes from 1 to epsilon\n",
    "    current_sparsity = 0.50*(sparsity_blend) + args.sparsity*(1. - sparsity_blend)\n",
    "    \n",
    "    #if True:\n",
    "    #  sparse_embedding_batch_squashed = SparseWinnerTakeAllLayer(\n",
    "    #                                      embedding_batch_middle, \n",
    "    #                                      sparsity=current_sparsity  ## Can't do variable indexing thing...\n",
    "    #                                    )\n",
    "    \n",
    "    sparse_embedding_batch_squashed = SparseWinnerTakeAllLayerApprox(\n",
    "                                        embedding_batch_middle, \n",
    "                                        approx_sparsity=current_sparsity\n",
    "                                      )\n",
    "    #sparsity_probe      \n",
    "    \n",
    "    \n",
    "elif args.mode == 'predict':\n",
    "  if version==20 or version==21 or version==22: # winner-take-all idea\n",
    "    if True:\n",
    "      embedding_batch_middle = lasagne.layers.batch_norm(\n",
    "          lasagne.layers.NonlinearityLayer( network,  nonlinearity=lasagne.nonlinearities.rectify )\n",
    "        )\n",
    "        \n",
    "    if version==20:\n",
    "      sparse_embedding_batch_squashed = SparseWinnerTakeAllLayer(\n",
    "                                          embedding_batch_middle, \n",
    "                                          sparsity=args.sparsity,\n",
    "                                          #deterministic=True\n",
    "                                        )\n",
    "    else:\n",
    "      sparse_embedding_batch_squashed = SparseWinnerTakeAllLayerApprox(\n",
    "                                          embedding_batch_middle, \n",
    "                                          approx_sparsity=args.sparsity,   # Jam the actual (final) value in...\n",
    "                                          #deterministic=True\n",
    "                                        )\n",
    "    \n",
    "\n",
    "else:\n",
    "  print(\"Need to know mode to do correct non-linearity\")\n",
    "  exit(0)\n",
    "\n",
    "if sparse_embedding_batch_probs is None:\n",
    "  sparse_embedding_batch_probs = sparse_embedding_batch_squashed\n",
    "\n",
    "network = sparse_embedding_batch_squashed\n",
    "\n",
    "if version==22:\n",
    "  network = lasagne.layers.DenseLayer(\n",
    "      network,\n",
    "      num_units=embedding_dim*2,\n",
    "      nonlinearity=lasagne.nonlinearities.rectify,\n",
    "      W=lasagne.init.GlorotUniform(),\n",
    "      b=lasagne.init.Constant(0.)\n",
    "    )\n",
    "\n",
    "network = lasagne.layers.DenseLayer(\n",
    "    network,\n",
    "    num_units=embedding_dim,\n",
    "    nonlinearity=lasagne.nonlinearities.linear,\n",
    "    W=lasagne.init.GlorotUniform(),\n",
    "    b=lasagne.init.Constant(0.)\n",
    "  )\n",
    "\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "\n",
    "l2_error = lasagne.objectives.squared_error( prediction, embedding_batch )   \n",
    "l2_error_mean = l2_error.mean()  # This is a per-element error term\n",
    "          \n",
    "#eps = .000001\n",
    "#sparsity_cost = theano.tensor.mean(theano.tensor.log(sparse_embedding_batch+eps) + theano.tensor.log(1.-eps - sparse_embedding_batch))\n",
    "\n",
    "#mix = 0.001\n",
    "#cost = (l2_error + mix*sparsity_cost).astype('float32')\n",
    "\n",
    "interim_output = lasagne.layers.get_output(sparse_embedding_batch_probs)\n",
    "if version==21 or version==22:  # Count the number of positive entries\n",
    "  sparse_flag = theano.tensor.switch( theano.tensor.ge(interim_output, 0.0001), 1.0, 0.0)\n",
    "  \n",
    "  #sparsity_mean  = sparse_flag.mean() / args.sparsity  # This is a number 0..1, where 1.0 = perfect = on-target\n",
    "  sparsity_mean  = sparse_flag.mean() * 100.  # This is realised sparsity \n",
    "\n",
    "  sparsity_std  = (sparse_flag.mean(axis=1) / args.sparsity).std()     # assess the 'quality' of the sparsity per-row\n",
    "\n",
    "  sparsity_probe = sparse_flag.mean(axis=1) / args.sparsity # sparsity across rows may not be ===1.0\n",
    "  #sparsity_probe = sparse_flag.mean(axis=0) / args.sparsity # sparsity across columns should be ===1.0 (if approximation works)\n",
    "\n",
    "\n",
    "else:\n",
    "  sparsity = theano.tensor.mean( (interim_output-0.5)**2 )\n",
    "  sparsity_mean = sparsity.mean() * 4.0  # This is a number 0..1, where 1=perfect, 0=terrible\n",
    "\n",
    "sparsity_cost=0.0\n",
    "if args.mode == 'train':\n",
    "  mix = theano.tensor.scalar(name='mix', dtype='float32')\n",
    "\n",
    "  #eps = .000001\n",
    "  #sparsity_cost = sigma * theano.tensor.mean(theano.tensor.log(interim_output+eps) + theano.tensor.log(1.-eps - interim_output))\n",
    "  \n",
    "  sparsity_cost = -mix*sparsity_mean/1000.  # The 1000 factor is because '10' l2 is Ok, and 1 sparsity_mean is Great\n",
    "  if version==20 or version==21:\n",
    "    sparsity_cost = mix*0.\n",
    "  \n",
    "cost = l2_error_mean + sparsity_cost\n",
    "\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "epoch_base=0\n",
    "if args.load:\n",
    "  load_vars = hickle.load(args.load)\n",
    "  print(\"Saved file had : Epoch:%4d, sigma:%5.2f\" % (load_vars['epoch'], load_vars['sigma'], ) )\n",
    "  #fraction_of_vocab=fraction_of_vocab\n",
    "  \n",
    "  epoch_base = load_vars['epoch']\n",
    "  \n",
    "  if 'layer_names' in load_vars:\n",
    "    layer_names = load_vars['layer_names']\n",
    "  else:\n",
    "    i=0\n",
    "    layer_names=[]\n",
    "    while \"Lasagne%d\" % (i,) in load_vars:\n",
    "      layer_names.append( \"Lasagne%d\" % (i,) )\n",
    "      i=i+1\n",
    "    \n",
    "  layers = [ load_vars[ ln ] for ln in layer_names ]\n",
    "  \n",
    "  lasagne.layers.set_all_param_values(network, layers)\n",
    "\n",
    "  \n",
    "if args.mode == 'train':\n",
    "  updates = lasagne.updates.adam( cost, params )\n",
    "\n",
    "  #iterate_net = theano.function( [batch_start_index], [l2_error_mean,sparsity_mean], updates=updates, \n",
    "  iterate_net = theano.function( \n",
    "                  [batch_start_index,sigma,mix], \n",
    "                  [l2_error_mean,sparsity_mean,sparsity_std,sparsity_probe], \n",
    "                  updates=updates, \n",
    "                  allow_input_downcast=True,\n",
    "                  on_unused_input='warn',\n",
    "                )\n",
    "\n",
    "  print(\"Built Theano op graph\")\n",
    "  \n",
    "  sigma_ = 0.0\n",
    "  mix_ = 0.0\n",
    "  if args.param:\n",
    "    mix_=args.param\n",
    "  \n",
    "  t0 = time.time()\n",
    "  for epoch in range(epoch_base, epoch_base+args.iters):\n",
    "    t1 = time.time()\n",
    "    \n",
    "    if version<8:\n",
    "      fraction_of_vocab = 0.1 + epoch*(0.05)\n",
    "      if fraction_of_vocab>1.0: \n",
    "        fraction_of_vocab=1.0\n",
    "\n",
    "      if epoch>20:\n",
    "        if epoch % 10 == 0:\n",
    "          sigma_ += 0.02\n",
    "      \n",
    "      if epoch>1000:\n",
    "        sigma_ = 2.0\n",
    "    \n",
    "    if version>=8:\n",
    "      fraction_of_vocab = 1.0\n",
    "\n",
    "    max_l2_error_mean=-1000.0\n",
    "\n",
    "    batch_list = np.array( range(0, int(embedding.shape[0]*fraction_of_vocab), batchsize) )\n",
    "    batch_list = np.random.permutation( batch_list )\n",
    "    \n",
    "    for b_start in batch_list.astype(int).tolist():\n",
    "      #l2_error_mean_,sparsity_mean_ = iterate_net(b_start)\n",
    "      \n",
    "      l2_error_mean_,sparsity_mean_,sparsity_std_,sparsity_probe_ = iterate_net(b_start, sigma_, mix_)\n",
    "\n",
    "      print(\" epoch:,%4d, b:,%7d, l2:,%9.2f, sparsity_mean_:,%9.4f, sparsity_std_:,%9.4f, sigma:,%5.2f, mix:,%5.2f, \" % \n",
    "          (epoch, b_start, 1000*l2_error_mean_, sparsity_mean_, sparsity_std_, sigma_, mix_, ))\n",
    "\n",
    "      if b_start==0:\n",
    "        #print(\"Hurdles : \" + np_int_list( sparsity_probe_[0:100] ))\n",
    "        print(\"  Row-wise sparsity : \" + np_int_list( sparsity_probe_[0:30] ))\n",
    "        #print(\"  %d, vector_probe : %s\" % (epoch, np_int_list( np.sort(sparsity_probe_[0:100]) ), )) \n",
    "        #print(\"  %d, vector_probe : %s\" % (epoch, np_int_list( sparsity_probe_[0:100] ), )) \n",
    "        #print(\"  vector_probe : \" + np_int_list( sparsity_probe_[0:1000] ))\n",
    "      \n",
    "      if max_l2_error_mean<l2_error_mean_:\n",
    "        max_l2_error_mean=l2_error_mean_\n",
    "\n",
    "    print(\"Time per 100k words %6.2fs\" % ((time.time() - t1)/embedding.shape[0]/fraction_of_vocab*1000.*100.,  ))\n",
    "    #exit()\n",
    "\n",
    "    boil_limit=10.\n",
    "    if version==14:\n",
    "      boil_limit=5.\n",
    "    \n",
    "    if args.normalize:\n",
    "      boil_limit=40.\n",
    "    \n",
    "    if max_l2_error_mean*1000.<boil_limit and version<99:\n",
    "      print(\"max_l2_error_mean<%6.2f - increasing sparseness emphasis\" % (boil_limit,))\n",
    "      if version<11 and sigma_<2.0 :\n",
    "        sigma_ += 0.01\n",
    "      if version>=11:\n",
    "        sigma_ += 0.01\n",
    "      mix_ += 0.1\n",
    "\n",
    "    if (epoch +1) % 10 == 0:\n",
    "      save_vars = dict(\n",
    "        version=version,\n",
    "        epoch=epoch,\n",
    "        sigma=sigma_,\n",
    "        mix=mix_,\n",
    "        fraction_of_vocab=fraction_of_vocab\n",
    "      )\n",
    "\n",
    "      layer_names = []\n",
    "      for i,p in enumerate(lasagne.layers.get_all_param_values(network)):\n",
    "        if len(p)>0:\n",
    "          name = \"Lasagne%d\" % (i,)\n",
    "          save_vars[ name ] = p\n",
    "          layer_names.append( name )\n",
    "      save_vars[ 'layer_names' ] = layer_names\n",
    "    \n",
    "      #epoch_thinned = epoch\n",
    "      #epoch_thinned = int(epoch/10)*10\n",
    "      #epoch_thinned = int(epoch/50)*50\n",
    "      epoch_thinned = int(epoch/100)*100\n",
    "      hickle.dump(save_vars, args.save % (epoch_thinned,), mode='w', compression='gzip')\n",
    "\n",
    "\n",
    "if args.load and args.mode == 'predict':\n",
    "  print(\"Parameters : \", lasagne.layers.get_all_params(network))\n",
    "  \n",
    "  get_sparse_linear = theano.function( [batch_start_index], [ lasagne.layers.get_output(sparse_embedding_batch_linear), ])  # allow_input_downcast=True \n",
    "  predict_net = theano.function( [batch_start_index], [l2_error_mean,sparsity_mean], allow_input_downcast=True )\n",
    "  predict_emb = theano.function( [batch_start_index], [prediction], allow_input_downcast=True )\n",
    "\n",
    "  predict_bin = theano.function( [batch_start_index], [ lasagne.layers.get_output(sparse_embedding_batch_squashed),])\n",
    "\n",
    "  print(\"Built Theano op graph\")\n",
    "\n",
    "  if True:  # Shows the error predictions with hard01 sigmoid\n",
    "    for b_start in range(0, int(embedding.shape[0]), batchsize):\n",
    "      l2_error_mean_,sparsity_mean_ = predict_net(b_start)\n",
    "\n",
    "      print(\" epoch:%4d, b:%7d, l2:%12.4f, sparsity:%6.4f - hard01\" % \n",
    "          (epoch_base, b_start, 1000*l2_error_mean_, sparsity_mean_, ))\n",
    "\n",
    "  if False:  # Shows the linear range of the sparse layer (pre-squashing)\n",
    "    for b_start in range(0, int(embedding.shape[0]), batchsize * 5):\n",
    "      sparse_embedding_batch_linear_, = get_sparse_linear(b_start)\n",
    "\n",
    "      for row in range(0,100,5):\n",
    "        print(np_int_list( sparse_embedding_batch_linear_[row][0:1000:50], mult=10, size=4 ))\n",
    "\n",
    "  if args.output:\n",
    "    predictions=[]\n",
    "    for b_start in range(0, int(embedding.shape[0]), batchsize):\n",
    "      prediction_, = predict_emb(b_start)\n",
    "      \n",
    "      predictions.append( np.array( prediction_ ) )\n",
    "\n",
    "      print(\" epoch:%3d, b:%7d, Downloading - reconstructed array\" % \n",
    "          (epoch_base, b_start, ))\n",
    "    \n",
    "    embedding_prediction = np.concatenate(predictions, axis=0)\n",
    "    predictions=None\n",
    "\n",
    "    print(\"About to save to %s\" % (args.output,))\n",
    "    d=dict( \n",
    "      vocab=vocab, \n",
    "      vocab_orig=vocab_orig,\n",
    "      embedding=embedding_prediction,\n",
    "    )\n",
    "    hickle.dump(d, args.output, mode='w', compression='gzip')\n",
    "  \n",
    "  if args.direct:\n",
    "    predictions=[]\n",
    "    for b_start in range(0, int(embedding.shape[0]), batchsize):\n",
    "      binarised_, = predict_bin(b_start)\n",
    "      \n",
    "      #predictions.append( np.where( binarised_>0.5, 1., 0. ).astype('float32') )\n",
    "      predictions.append( binarised_.astype('float32') )\n",
    "\n",
    "      #print(\" epoch:%3d, b:%7d, Downloading - hard01 to binary\" % \n",
    "      print(\" epoch:%3d, b:%7d, Downloading - sparse data\" % \n",
    "          (epoch_base, b_start, ))\n",
    "    \n",
    "    embedding_prediction = np.concatenate(predictions, axis=0)\n",
    "    predictions=None\n",
    "\n",
    "    print(\"About to save sparse version to %s\" % (args.direct,))\n",
    "    d=dict( \n",
    "      vocab=vocab, \n",
    "      vocab_orig=vocab_orig,\n",
    "      embedding=embedding_prediction,\n",
    "    )\n",
    "    hickle.dump(d, args.direct, mode='w', compression='gzip')\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_file = '../../data/2-pretrained-vectors/glove.6B.300d.hkl'\n",
    "#embedding_file = '../../data/1-glove-1-billion-and-wiki/window11-lc-36/vectors.2-17.hkl'\n",
    "#embedding_file = '../4-sparse/sparse.6B.300d_S-21_2n-shuf_4096@1.50_2000_GPU-sparsity_recreate.hkl'\n",
    "#embedding_file = '../4-sparse/sparse.6B.300d_S-21_2n-shuf_4096@1.50_2000_GPU-sparse_matrix.hkl'\n",
    "\n",
    "#embedding_file = '../4-sparse/sparse.6B.300d_S-21_2n-shuf_1024@6.75_2000_GPU-sparsity_recreate.hkl'\n",
    "#embedding_file = '../4-sparse/sparse.6B.300d_S-21_2n-shuf_1024@6.75_2000_GPU-sparse_matrix.hkl'\n",
    "\n",
    "#embedding_file = '../4-sparse/sparse.6B.300d_S-21_2n-shuf-noise-after-norm_4k_.2.01_1-50_5000_GPU-sparse_matrix.hkl'\n",
    "embedding_file = '../4-sparse/sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_4000_GPU-sparse_matrix.hkl'\n",
    "\n",
    "import numpy as np\n",
    "import hickle\n",
    "\n",
    "d = hickle.load(embedding_file)\n",
    "vocab, embedding = d['vocab'], d['embedding']\n",
    "vocab_orig = d['vocab_orig']\n",
    "\n",
    "dictionary = dict( (word, i) for i,word in enumerate(vocab) if i<len(embedding) )\n",
    "dictionary_orig = dict( (word, i) for i,word in enumerate(vocab_orig) if i<len(embedding) )\n",
    "\n",
    "print(\"Embedding loaded :\", embedding.shape)   # (vocab_size, embedding_dimension)=(rows, columns)\n",
    "embedding_normed = embedding / np.linalg.norm(embedding, axis=1)[:, np.newaxis]\n",
    "\n",
    "vocab[0]\n",
    "entries = [ x for x in embedding[0].tolist() if x!=0.0 ]\n",
    "len(entries)\n",
    "#45 \n",
    "\n",
    "for w in 'the iraq baghdad uk london criminal apple some hypothesis maximal innocuous'.split(' '):\n",
    "  i=dictionary[w]\n",
    "  entries = [ x for x in embedding[i].tolist() if x!=0.0 ]\n",
    "  print(\"%20s @%6d len=%d\" % (w,dictionary_orig[w],len(entries),))\n",
    "\n",
    "  #               the @     0 len=18\n",
    "  #              some @    60 len=18\n",
    "  #            london @   266 len=91\n",
    "  #                uk @   448 len=82\n",
    "  #              iraq @   606 len=113\n",
    "  #          criminal @  1449 len=104\n",
    "  #             apple @  2046 len=112\n",
    "  #           baghdad @  2320 len=116\n",
    "  #        hypothesis @  6957 len=136\n",
    "  #           maximal @ 27962 len=107\n",
    "  #         innocuous @ 30111 len=86\n",
    "\n",
    "\n",
    "# Look at per-position best words\n",
    "for i in range(0, embedding.shape[1], 10):\n",
    "  best_words_j = np.argsort( -embedding[:, i ] )[0:10]\n",
    "  for j in best_words_j:\n",
    "    print(\"%4i -> %s\" % (i, vocab[j],))\n",
    "  print('')\n",
    "\n",
    "i=2000\n",
    "values = [x for x in (-np.sort( -embedding[i] )).tolist() if x>0. ]\n",
    "print(\"values: [\"+', '.join([ ('%.4f' % (x,)) for x in values ])+']')\n",
    "#values: [1.1442, 0.9337, 0.9333, 0.9257, 0.7520, 0.5529, 0.4818, 0.4740, 0.4568, 0.4554, 0.4434, 0.4419, 0.4334, 0.4187, 0.4175, 0.4068, 0.4005, 0.3989, 0.3698, 0.3421, 0.3206, 0.3151, 0.3150, 0.3120, 0.3119, 0.3067, 0.3010, 0.2948, 0.2853, 0.2828, 0.2816, 0.2815, 0.2799, 0.2793, 0.2764, 0.2714, 0.2636, 0.2570, 0.2507, 0.2487, 0.2336, 0.2336, 0.2335, 0.2328, 0.2325, 0.2323, 0.2255, 0.2227, 0.2227, 0.2226, 0.2208, 0.2178, 0.2159, 0.2134, 0.2067, 0.2049, 0.1947, 0.1935, 0.1932, 0.1926, 0.1921, 0.1914, 0.1897, 0.1894, 0.1832, 0.1782, 0.1766, 0.1730, 0.1714, 0.1683, 0.1662, 0.1638, 0.1629, 0.1602, 0.1568, 0.1561, 0.1452, 0.1419, 0.1399, 0.1372, 0.1370, 0.1352, 0.1350, 0.1342, 0.1334, 0.1334, 0.1302, 0.1289, 0.1268, 0.1243, 0.1230, 0.1211, 0.1192, 0.1113, 0.1051]\n",
    "\n",
    "print(\"changes: [\"+', '.join([ ('%.1f' % (values[i+1]/values[i]*100.,)) for i in range(0,len(values)-1) ])+']')\n",
    "#changes: [81.6, 100.0, 99.2, 81.2, 73.5, 87.1, 98.4, 96.4, 99.7, 97.3, 99.7, 98.1, 96.6, 99.7, 97.4, 98.4, 99.6, 92.7, 92.5, 93.7, 98.3, 100.0, 99.0, 100.0, 98.3, 98.1, 97.9, 96.8, 99.1, 99.6, 100.0, 99.4, 99.8, 99.0, 98.2, 97.1, 97.5, 97.6, 99.2, 93.9, 100.0, 100.0, 99.7, 99.9, 99.9, 97.1, 98.8, 100.0, 100.0, 99.2, 98.6, 99.1, 98.9, 96.9, 99.1, 95.0, 99.4, 99.9, 99.7, 99.8, 99.6, 99.1, 99.8, 96.7, 97.3, 99.1, 98.0, 99.1, 98.2, 98.8, 98.6, 99.4, 98.3, 97.9, 99.5, 93.1, 97.7, 98.6, 98.1, 99.8, 98.7, 99.9, 99.4, 99.4, 100.0, 97.6, 99.0, 98.4, 98.0, 99.0, 98.4, 98.5, 93.4, 94.4]\n",
    "\n",
    "\n",
    "w='motorcycle'\n",
    "w_i=dictionary[w]\n",
    "\n",
    "#top_i =np.argmax(embedding[w_i])\n",
    "good_i =np.argsort( -embedding[w_i] )\n",
    "\n",
    "for i in range(0,10):\n",
    "  best_words_j = np.argsort( -embedding[:, good_i[i] ] )[0:12]\n",
    "  \n",
    "  #for j in best_words_j:\n",
    "  #  print(\"%s\" % (vocab[j],))\n",
    "  #print('')\n",
    "  \n",
    "  print(\"%s\" % (', '.join( [ vocab[j] for j in best_words_j] ), ) )\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_for(w):\n",
    "  w_i=dictionary[w]\n",
    "  return embedding[w_i]\n",
    "\n",
    "def l2_normed(e):\n",
    "  return e / np.sqrt( np.dot(e,e) )\n",
    "\n",
    "def cosine(a,b):\n",
    "  return np.dot(l2_normed(a), l2_normed(b))\n",
    "\n",
    "def top_senses_for(e):\n",
    "  good_i = np.argsort( -e )\n",
    "  for i in range(0,10):\n",
    "    best_words_j = np.argsort( -embedding[:, good_i[i] ] )[0:12]\n",
    "    print(\"%s\" % (', '.join( [ vocab[j] for j in best_words_j] ), ) )\n",
    "\n",
    "def closest_to(e):\n",
    "  closest = np.argsort( - np.dot(embedding_normed, l2_normed(e) ) )\n",
    "  return \"%s\" % (', '.join( [ vocab[j] for j in closest[0:20] ] ), ) \n",
    "\n",
    "def count_positive(e):\n",
    "  return len( [ x for x in e.tolist() if x>0.0 ] )\n",
    "\n",
    "def nonneg(e):\n",
    "  return np.maximum(0, e)\n",
    "\n",
    "def closest_dist(s):\n",
    "  ab,xy = s.split('=')\n",
    "  (a,b),(x,y) = ab.split(':'), xy.split(':')\n",
    "  print( \"%s is to %s as %s is to ?%s? \" % (a,b,x,y,))\n",
    "  (a,b,x,y) = map(vector_for, [a,b,x,y])  # Convert to vectors\n",
    "  print('  x+b-a           = %s' % (closest_to( x + b - a ),))\n",
    "  print('  [x+b-a]         = %s' % (closest_to( nonneg(x + b - a) ),))\n",
    "  print('  x+[b-a]         = %s' % (closest_to( x + nonneg(b-a) ),))\n",
    "  print('  [x-a]+b         = %s' % (closest_to( nonneg(x-a) + b ),))\n",
    "  print('  [2x-a]+[2b-a]   = %s' % (closest_to( nonneg(2*x-a) + nonneg(2*b-a) ),))\n",
    "  print('  x+[b-a]+b+[x-a] = %s' % (closest_to( x+nonneg(b-a) + b+nonneg(x-a) ),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_senses_for(vector_for('motorbike'))\n",
    "\n",
    "man   = vector_for('man')\n",
    "woman = vector_for('woman')\n",
    "king  = vector_for('king')\n",
    "queen = vector_for('queen')\n",
    "\n",
    "top_senses_for(man)\n",
    "top_senses_for(woman)\n",
    "top_senses_for(king)\n",
    "top_senses_for(queen)\n",
    "\n",
    "top_senses_for(man * woman) # Intersection\n",
    "top_senses_for(man + woman) # Union\n",
    "top_senses_for(man - woman) # ??\n",
    "\n",
    "\n",
    ">>> closest_to(man)\n",
    "man, woman, girl, person, men, teenager, she, friend, he, father, her, boy, someone, mother, him, his, victim, son, who, guy\n",
    ">>> closest_to(woman)\n",
    "woman, man, girl, mother, teenager, daughter, wife, women, her, person, she, girlfriend, friend, men, husband, widow, couple, boy, someone, victim\n",
    "\n",
    ">>> closest_to(king)\n",
    "king, queen, henry, mswati, mongkut, eirik, charles, vajiravudh, thoden, wenceslaus, zvonimir, athelstan, vladislaus, thelred, gojong, prince, jayavarman, kalkaua, sweyn, pomare\n",
    ">>> closest_to(queen)\n",
    "queen, princess, elizabeth, king, margrethe, empress, lady, sister, prince, sirikit, mary, cixi, monarch, daughter, duchess, olten, mother, infanta, rania, widow\n",
    "\n",
    "closest_dist('pound:england=franc:france')\n",
    "\n",
    "\n",
    "england,pound,america,dollar = map(vector_for, 'england pound america dollar'.split())\n",
    "\n",
    "curr = england,pound,america,dollar = map(vector_for, 'england pound america dollar'.split())\n",
    "map(count_positive, curr)\n",
    "[84, 126, 94, 134]\n",
    "\n",
    "map(count_positive, [ england*pound, america*dollar, england*america, pound*dollar])\n",
    "[12, 14, 17, 56]\n",
    "\n",
    "total = england+pound+america+dollar\n",
    "map(count_positive, [ england*101-100*total, pound*101-100*total, america*101-100*total, dollar*101-100*total])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
