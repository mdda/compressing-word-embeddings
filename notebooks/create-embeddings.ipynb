{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressing Word Embeddings\n",
    "\n",
    "Downloadable version of GloVe embedding (with fallback source).\n",
    "\n",
    "Then require two main sections : \n",
    " \n",
    "*  Lloyd embedding generation\n",
    "\n",
    "*  Sparsified embedding generation\n",
    "\n",
    "and then saving of the created embeddings to ```.hkl``` files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Source Embedding(s)\n",
    "\n",
    "The following needs to be Pythonized :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RCL_BASE=('http://redcatlabs.com/downloads/'+\n",
    "          'deep-learning-workshop/notebooks/data/'+\n",
    "          'research/ICONIP-2016/')\n",
    "\n",
    "\"\"\"\n",
    "# http://redcatlabs.com/downloads/deep-learning-workshop/LICENSE\n",
    "\n",
    "# Files in : ${RCL_BASE} :\n",
    "\n",
    "# :: These are either as downloaded from GloVe site, or generated by Levy code\n",
    "# 507206240 Oct 25  2015 2-pretrained-vectors_glove.6B.300d.hkl\n",
    "# 160569440 May 14 14:57 1-glove-1-billion-and-wiki_window11-lc-36_vectors.2-17.hkl\n",
    "\"\"\"\n",
    "\n",
    "import os, requests\n",
    "\n",
    "def get_embedding_file( hkl ):  \n",
    "    if not os.path.isfile(os.path.join('data', hkl)):\n",
    "        # ... requests.get( RCL_BASE + hkl)\n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_embedding_file = '../data/2-pretrained-vectors_glove.6B.300d.hkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import theano\n",
    "import lasagne\n",
    "\n",
    "# http://blog.mdda.net/oss/2016/04/07/nvidia-on-fedora-23\n",
    "#theano.config.nvcc.flags = '-D_GLIBCXX_USE_CXX11_ABI=0'\n",
    "\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import hickle\n",
    "\n",
    "d = hickle.load(default_embedding_file)\n",
    "vocab, embedding = d['vocab'], d['embedding']\n",
    "\n",
    "vocab_np = np.array(vocab, dtype=str)\n",
    "vocab_orig=vocab_np.copy()\n",
    "\n",
    "#dictionary = dict( (word.lower(), i) for i,word in enumerate(vocab) )\n",
    "dictionary = dict( (word, i) for i,word in enumerate(vocab) if i<len(embedding) )\n",
    "\n",
    "print(\"Embedding loaded :\", embedding.shape)   # (vocab_size, embedding_dimension)=(rows, columns)\n",
    "\n",
    "def NO_NEED_save_to_txt(embedding_save, save_filename_txt):\n",
    "  with open(save_filename_txt, 'wb') as f:\n",
    "    embedding_save = embedding_normed\n",
    "    for l in range(0, embedding_save.shape[0]):\n",
    "      f.write(\"%s %s\\n\" % (\n",
    "          vocab[l], \n",
    "          ' '.join([ ('0' if x==0. else (\"%.6f\" % (x,))) for x in embedding_save[l, :].tolist() ]), )\n",
    "      )\n",
    "\n",
    "def save_embedding_to_hickle(vocab, embedding_save, save_filename_hkl, vocab_orig=None):\n",
    "  print(\"About to save to %s\" % (save_filename_hkl,))\n",
    "  d=dict( \n",
    "    vocab=vocab, \n",
    "    vocab_orig=vocab if vocab_orig is None else vocab,\n",
    "    embedding=embedding_save,\n",
    "  )\n",
    "  hickle.dump(d, save_filename_hkl, mode='w', compression='gzip')\n",
    "  print(\"Saved to %s\" % (save_filename_hkl,))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lloyd's Method : 32->3 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quantisation_levels = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def np_int_list(n, mult=100., size=3):  # size includes the +/-\n",
    "  return \"[ \" + (', '.join([ ('% +*d') % (size,x,) for x in (n * mult).astype(int).tolist()])) + \" ]\"\n",
    "\n",
    "## Quantise each entry into 'pct' (as an integer) level (optimised per vector location)\n",
    "#    Suppose that v is a vector of levels\n",
    "#    and c is a list of numbers that needs to be quantised, \n",
    "#    each c becomes c' where c' is the closest value in v\n",
    "#      :: update v so that (c - c')^2 is as low as possible\n",
    "\n",
    "c_length = embedding.shape[0]\n",
    "\n",
    "embedding_quantised = np.zeros_like(embedding)\n",
    "\n",
    "t0 = time.time()\n",
    "for d in range(embedding.shape[1]):   # Quantise each dimension separately\n",
    "  levels = quantisation_levels\n",
    "\n",
    "  i_step = int(c_length/levels)\n",
    "  i_start = int(i_step/2)\n",
    "\n",
    "  v_indices = np.arange(start=i_start, stop=c_length, step=i_step, dtype='int')\n",
    "\n",
    "  #if d != 9: continue  # Weird distribution\n",
    "  #if d != 1: continue  # Very standard example\n",
    "\n",
    "  # Initialise v by sorting c, and placing them evenly through the list\n",
    "  e_column = embedding[:,d].astype('float32')\n",
    "\n",
    "  c_sorted = np.sort( e_column )\n",
    "  v_init = c_sorted[ v_indices ]\n",
    "\n",
    "  # the v_init are the initial centers \n",
    "  v=v_init\n",
    "\n",
    "  t1 = time.time()\n",
    "  epochs=0\n",
    "  for epoch in range(0, 1000):\n",
    "    #print(\" Dimension:%3d, Epoch:%3d, %s\" % (d, epoch, np_int_list(v),))\n",
    "\n",
    "    #   works out the values in their middles\n",
    "    mids_np = (v[:-1] + v[1:])/2.\n",
    "\n",
    "    mids = mids_np.tolist()\n",
    "    mids.insert( 0, c_sorted[0] )\n",
    "    mids.append( c_sorted[-1] +1 )\n",
    "\n",
    "    centroids=[]\n",
    "    for i in range( 0, len(mids)-1 ):\n",
    "      pattern = np.where( (mids[i] <= c_sorted) & (c_sorted < mids[i+1]) )\n",
    "      centroids.append( c_sorted[ pattern ].mean() )\n",
    "\n",
    "    centroids_np = np.array(centroids)\n",
    "\n",
    "    if np.allclose(v, centroids_np):\n",
    "      if epochs>200: # This only prints out for 'long convergence cases'\n",
    "        print(\"  NB : long running convergence : embedding[%3d] - took %d epochs\" % (d, epochs,))\n",
    "      break\n",
    "\n",
    "    v = centroids_np\n",
    "\n",
    "    epochs += 1\n",
    "\n",
    "  if d % 10 ==0:\n",
    "    print(\"Ran embedding[%3d] - average time for convergence : %6.2fms\" % (d, (time.time() - t1)/epochs*1000.,))\n",
    "\n",
    "\n",
    "  #print(\"Check col updated: before \", np_int_list(embedding[0:20,d]))\n",
    "\n",
    "  # Ok, so now we have the centers in v, and the mids in 'mids'\n",
    "  for i in range( 0, len(mids)-1 ):\n",
    "    pattern = np.where( (mids[i] <= e_column) & (e_column < mids[i+1]) )\n",
    "    embedding_quantised[pattern, d] = v[i]\n",
    "\n",
    "  #print(\"Check col updated: after  \", np_int_list(embedding_quantised[0:20,d]))\n",
    "\n",
    "if False:\n",
    "  offset=101010  # Check rare-ish words\n",
    "  for d in range(5, embedding_quantised.shape[1], 25):\n",
    "    print(\"Col %3d updated: \" % (d,), np_int_list(embedding_quantised[(offset+0):(offset+20),d]))\n",
    "\n",
    "embedding_normed = sklearn.preprocessing.normalize(embedding_quantised, norm='l2', axis=1, copy=True) \n",
    "print(\"Quantisation finished : results in embedding_quantised and (same, but normalised) in embedding_normed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the created embedding, execute the following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the embedding_normed as a hickle file (easy to reload into the 'explore' workbook)\n",
    "save_embedding_to_hickle(vocab, embedding_normed, '../data/lloyds_normed_%d.hkl' % (quantisation_levels, ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Negative Sparse Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python sparsify_lasagne.py \n",
    "       --mode=train       \\\n",
    "       --version=21       \\\n",
    "       --save='./sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_%04d.hkl'  \\\n",
    "       --sparsity=0.0675  \\\n",
    "       --random=1         \\\n",
    "       --iters=4000 | tee sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75.log\n",
    "      #sparse_dim = 1024, pre-num_units=embedding_dim*8,   \n",
    "```\n",
    "      \n",
    "```\n",
    "# -> 4.0 l2 in 4.0k epochs (sigma=39)  # sparsity_std_:,   0.4742,\n",
    "python sparsify_lasagne.py \n",
    "      --mode=predict \\\n",
    "      --version=21 \\\n",
    "      --load='./sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_4000.hkl' \\\n",
    "      --sparsity=0.0675 \\\n",
    "      --random=1 \\\n",
    "      --output=sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_4000_GPU-sparsity_recreate.hkl \\\n",
    "      --direct=sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_4000_GPU-sparse_matrix.hkl \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shuffle_vocab = True\n",
    "batchsize = 16384  # (GTX760 requires <20000)\n",
    "\n",
    "sparse_dim,sparsity_goal = 1024, 0.0675\n",
    "#sparse_dim,sparsity_goal = 4096, 0.0150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_save_file_fmt  = './data/sparse.6B.300d_jupyter_%%04d.hkl'\n",
    "\n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser(description='')\n",
    "parser.add_argument('-m','--mode', help='(train|predict)', type=str, default=None)\n",
    "\n",
    "parser.add_argument('-i','--iters', help='Number of iterations', type=int, default=10000)\n",
    "\n",
    "parser.add_argument('-o','--output', help='hickle to *create* embedding for testing', type=str, default=None)\n",
    "parser.add_argument('-d','--direct', help='hickle to *create* *binary* embedding for testing', type=str, default=None)\n",
    "\n",
    "parser.add_argument('-p','--param', help='Set param value initially', type=float, default=None)\n",
    "parser.add_argument('-k','--sparsity',  help='Sparsity value goal', type=float, default=0.05)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(\"Mode : %s\" % (args.mode,)) \n",
    "\"\"\"\n",
    "\n",
    "if shuffle_vocab:\n",
    "   np.random.seed(1) # No need to get fancy - just want to mix up the word frequencies into different batches\n",
    "   perm = np.random.permutation(len(embedding))\n",
    "   embedding = embedding[perm]\n",
    "   vocab = vocab_np[perm].tolist()\n",
    "  \n",
    "dictionary = dict( (word, i) for i,word in enumerate(vocab) )\n",
    "\n",
    "print(\"Embedding loaded :\", embedding.shape)   # (vocab_size, embedding_dimension)=(rows, columns)\n",
    "print(\"Device=%s, OpenMP=%s\" % (theano.config.device, (\"True\" if theano.config.openmp else \"False\"), ))\n",
    "\n",
    "def np_int_list(n, mult=100., size=3):  # size includes the +/-\n",
    "  return \"[ \" + (', '.join([ ('% +*d') % (size,x,) for x in (n * mult).astype(int).tolist()])) + \" ]\"\n",
    "\n",
    "embedding_dim = embedding.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "class SparseWinnerTakeAllLayer(lasagne.layers.Layer):\n",
    "    def __init__(self, incoming, sparsity=0.05, **kwargs):\n",
    "        super(SparseWinnerTakeAllLayer, self).__init__(incoming, **kwargs)\n",
    "        self.sparsity = sparsity\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : tensor\n",
    "            output from the previous layer\n",
    "        \"\"\"\n",
    "        # Sort within batch (Very likely on the CPU)\n",
    "        # theano.tensor.sort(self, axis, kind, order)\n",
    "        sort_input = input.sort( axis=0, kind='quicksort' )\n",
    "\n",
    "        # Find kth value\n",
    "        hurdles_raw = sort_input[ int( batchsize * (1.0 - self.sparsity) ), : ]\n",
    "        hurdles = theano.tensor.maximum(hurdles_raw, 0.0)  # rectification...\n",
    "\n",
    "        # switch based on >kth value (or create mask), all other entries are zero\n",
    "        masked = theano.tensor.switch( theano.tensor.ge(input, hurdles), input, 0.0)\n",
    "        return masked\n",
    "        \n",
    "class SparseWinnerTakeAllLayerApprox(lasagne.layers.Layer):\n",
    "    def __init__(self, incoming, approx_sparsity=0.12, **kwargs):  \n",
    "        super(SparseWinnerTakeAllLayerApprox, self).__init__(incoming, **kwargs)\n",
    "        self.sparsity = approx_sparsity\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : tensor\n",
    "            output from the previous layer\n",
    "        \"\"\"\n",
    "        # input_shape is [ #in_batch, #vector_entries ] ~ [ 20k, 1024 ]\n",
    "    \n",
    "        current_sparsity = self.sparsity\n",
    "        #print(current_sparsity)  # A theano variable\n",
    "        \n",
    "        if False:\n",
    "          # This is an 'advanced' tail-aware hurdle-level prediction.  \n",
    "          #   In the end, it works less well than the binary-search version below\n",
    "            \n",
    "          # Find the max value in each column - this is the k=1 (top-most) entry\n",
    "          hurdles_max  = input.max( axis=0 )\n",
    "          \n",
    "          input = lasagne.layers.get_output(embedding_batch_middle)\n",
    "          \n",
    "          # Find the max value in each column - this is the k=1 (top-most) entry\n",
    "          hurdles_max  = input.max( axis=0 )\n",
    "          \n",
    "          # Find the min value in each column - this is the k=all (bottom-most) entry\n",
    "          #hurdles_min  = input.min( axis=0 )\n",
    "\n",
    "          # Let's guess (poorly) that the sparsity hurdle is (0... sparsity ...100%) within these bounds\n",
    "          #hurdles_guess = hurdles_max * (1.0 - current_sparsity) + hurdles_min * current_sparsity\n",
    "          \n",
    "          #hurdles_guess = (hurdles_min + hurdles_max)/2.0\n",
    "          \n",
    "          # New approach : We know that the mean() is zero and the std() is 1\n",
    "          #   simulations suggest that the more stable indicators are at fractions of the max()\n",
    "          \n",
    "          hurdles_hi = hurdles_max * 0.5\n",
    "          hurdles_lo = hurdles_max * 0.3\n",
    "          \n",
    "          # Now, let's find the actual sparsity that this creates\n",
    "          sparsity_flag_hi = theano.tensor.switch( theano.tensor.ge(input, hurdles_hi), 1.0, 0.0)\n",
    "          sparsity_real_hi = sparsity_flag_hi.mean(axis=0)    # Should be ~ sparsity (likely to be lower, though)\n",
    "\n",
    "          sparsity_flag_lo = theano.tensor.switch( theano.tensor.ge(input, hurdles_lo), 1.0, 0.0)\n",
    "          sparsity_real_lo = sparsity_flag_lo.mean(axis=0)    # Should be ~ sparsity (likely to be higher, though)\n",
    "          \n",
    "          # But this is wrong!  Let's do another estimate (will be much closer, hopefully) using this knowledge\n",
    "          #   For each column, the new hurdle guess\n",
    "          \n",
    "          #hurdles_better = hurdles_max - ( current_sparsity / (sparsity_guess_real + 0.00001) ) * (hurdles_max - hurdles_guess)\n",
    "          \n",
    "\n",
    "          if False: # This assumes that the distribution tails are linear (which is not true)\n",
    "            hurdles_interp = hurdles_hi + (hurdles_lo-hurdles_hi) * (current_sparsity - sparsity_real_hi) / ((sparsity_real_lo - sparsity_real_hi)+0.00001)\n",
    "            \n",
    "          else:  # Assume that the areas under the tails are ~ exp(-x*x)  \n",
    "            # See (2) in : https://math.uc.edu/~brycw/preprint/z-tail/z-tail.pdf\n",
    "            # *** See (Remark 15) in : http://m-hikari.com/ams/ams-2014/ams-85-88-2014/epureAMS85-88-2014.pdf\n",
    "            \n",
    "            def tail_transform(z):\n",
    "              return theano.tensor.sqrt( -theano.tensor.log( z ) )\n",
    "            \n",
    "            tail_target = tail_transform(current_sparsity)\n",
    "            tail_hi = tail_transform(sparsity_real_hi)\n",
    "            tail_lo = tail_transform(sparsity_real_lo)\n",
    "\n",
    "            hurdles_interp = hurdles_hi + (hurdles_lo-hurdles_hi) * (tail_target - tail_hi) / ((tail_lo - tail_hi)+0.00001)\n",
    "          \n",
    "          #hurdles = theano.tensor.maximum(hurdles_better, 0.0)  # rectification... at mininim... (also solves everything-blowing-up problem)\n",
    "          hurdles = hurdles_interp.clip(hurdles_max*0.2, hurdles_max*0.9)\n",
    "\n",
    "\n",
    "        if True:  # Simple, but effective : Binary search\n",
    "          hurdles_hi, hurdles_lo = [], []\n",
    "          \n",
    "          hurdles_guess = []\n",
    "          sparsity_flag = []\n",
    "          sparsity_real = []\n",
    "          \n",
    "          sparsity_hi, sparsity_lo = [], []\n",
    "\n",
    "          # Find the max value in each column - this is the k=1 (top-most) entry\n",
    "          hurdles_max  = input.max( axis=0 )\n",
    "          \n",
    "          hurdles_hi.append(hurdles_max)\n",
    "          sparsity_hi.append(hurdles_max * (1./batchsize) ) \n",
    "          \n",
    "\n",
    "          hurdles_lo_temp = input.mean( axis=0 )  # Different estimate idea...\n",
    "\n",
    "          hurdles_lo.append(hurdles_lo_temp)\n",
    "          sparsity_lo_temp = theano.tensor.switch( theano.tensor.ge(input, hurdles_lo_temp), 1.0, 0.0)\n",
    "          sparsity_lo.append( sparsity_lo_temp.mean(axis=0) )\n",
    "          \n",
    "          for i in range(10):  \n",
    "            if True:   # WINS THE DAY!\n",
    "              hurdles_guess.append(\n",
    "                (\n",
    "                  (hurdles_lo[-1] + hurdles_hi[-1]) * 0.5\n",
    "                )\n",
    "              )\n",
    "\n",
    "            if False:  # A 'better approximation' that is actually worse\n",
    "              hurdles_guess.append(\n",
    "                (\n",
    "                  hurdles_hi[-1] + (hurdles_lo[-1] - hurdles_hi[-1]) * \n",
    "                    (current_sparsity - sparsity_hi[-1]) / ((sparsity_lo[-1] - sparsity_hi[-1])+0.000001)\n",
    "                ).clip(hurdles_lo[-1], hurdles_hi[-1])\n",
    "              )\n",
    "\n",
    "            if False:  # Another 'better approximation' that is actually worse\n",
    "              # switch on closeness to getting it correct\n",
    "              hurdles_guess.append(\n",
    "                theano.tensor.switch( theano.tensor.lt( sparsity_lo[-1], current_sparsity * 2.0 ),\n",
    "                  (\n",
    "                    hurdles_hi[-1] + (hurdles_lo[-1] - hurdles_hi[-1]) * \n",
    "                      (current_sparsity - sparsity_hi[-1]) / ((sparsity_lo[-1] - sparsity_hi[-1])+0.000001)\n",
    "                  ).clip(hurdles_lo[-1], hurdles_hi[-1]),\n",
    "                  (\n",
    "                    (hurdles_lo[-1] + hurdles_hi[-1]) * 0.5\n",
    "                  )\n",
    "                )\n",
    "                \n",
    "              )\n",
    "              \n",
    "            \n",
    "            sparsity_flag.append( theano.tensor.switch( theano.tensor.ge(input, hurdles_guess[-1] ), 1.0, 0.0) )\n",
    "            sparsity_real.append( sparsity_flag[-1].mean(axis=0) )\n",
    "            \n",
    "            # So, based on whether the real sparsity is greater or less than the real value, change the hi or lo values\n",
    "\n",
    "            hurdles_lo.append( \n",
    "              theano.tensor.switch( theano.tensor.gt(current_sparsity, sparsity_real[-1]), \n",
    "                                   hurdles_lo[-1], hurdles_guess[-1]) \n",
    "            )\n",
    "            hurdles_hi.append( \n",
    "              theano.tensor.switch( theano.tensor.le(current_sparsity, sparsity_real[-1]), \n",
    "                                   hurdles_hi[-1], hurdles_guess[-1]) \n",
    "            )\n",
    "\n",
    "          hurdles = hurdles_guess[-1]\n",
    "          #hurdles = hurdles_lo[-1]  # Better to bound this at the highest relevant sparsity...\n",
    "          \n",
    "        masked = theano.tensor.switch( theano.tensor.ge(input, hurdles), input, 0.0)\n",
    "        return masked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_N = (embedding)  # No Normalization by default\n",
    "\n",
    "if False: # args.normalize:\n",
    "  #>>> a=np.array( [ [1,-1,1,-1], [-5,5,5,-5] ])\n",
    "  #>>> b=np.std(a, axis=1)\n",
    "  #>>> a / b[:, np.newaxis]\n",
    "  #array([[ 1., -1.,  1., -1.],\n",
    "  #       [-1.,  1.,  1., -1.]])\n",
    "  \n",
    "  embedding_std  = np.std(embedding, axis=1)\n",
    "  embedding_N = embedding / embedding_std[:, np.newaxis]    # Try Normalizing  std(row) == 1, making sure shapes are right\n",
    "\n",
    "\n",
    "embedding_shared = theano.shared(embedding_N.astype('float32'))       # 400000, 300\n",
    "embedding_shared.name = \"embedding_shared\"\n",
    "\n",
    "batch_start_index = theano.tensor.scalar('batch_start_index', dtype='int32')\n",
    "\n",
    "embedding_batch = embedding_shared[ batch_start_index:(batch_start_index+batchsize) ]\n",
    "\n",
    "network = lasagne.layers.InputLayer( \n",
    "    ( batchsize, embedding_dim ), \n",
    "    input_var=embedding_batch,\n",
    "  )\n",
    "\n",
    "pre_hidden_dim=embedding_dim*8  ## For sparse_dim=1024 and below\n",
    "if sparse_dim>1024*1.5:\n",
    "  pre_hidden_dim=sparse_dim*2   ## Larger sparse_dim\n",
    "\n",
    "network = lasagne.layers.DenseLayer(\n",
    "    network,\n",
    "    num_units=pre_hidden_dim,     \n",
    "    nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    W=lasagne.init.GlorotUniform(),\n",
    "    b=lasagne.init.Constant(0.)\n",
    "  )\n",
    "\n",
    "#if version==22:\n",
    "#  network = lasagne.layers.DenseLayer(\n",
    "#      network,\n",
    "#      num_units=sparse_dim*2,\n",
    "#      nonlinearity=lasagne.nonlinearities.rectify,\n",
    "#      W=lasagne.init.GlorotUniform(),\n",
    "#      b=lasagne.init.Constant(0.)\n",
    "#    )\n",
    "\n",
    "network = lasagne.layers.DenseLayer(\n",
    "    network,\n",
    "    num_units=sparse_dim,\n",
    "    nonlinearity=lasagne.nonlinearities.identity,\n",
    "    W=lasagne.init.GlorotUniform(),\n",
    "    b=lasagne.init.Constant(0.)\n",
    "  )\n",
    "\n",
    "sparse_embedding_batch_linear=network\n",
    "\n",
    "def hard01(x):\n",
    "  # http://deeplearning.net/software/theano/library/tensor/basic.html#theano.tensor.switch\n",
    "  #return theano.tensor.switch( theano.tensor.gt(x, 0.), 0.95, 0.05)\n",
    "  return theano.tensor.switch( theano.tensor.gt(x, 0.), 1.0, 0.0)\n",
    "  \n",
    "sparse_embedding_batch_probs=None\n",
    "if args.mode == 'train':\n",
    "  sigma = theano.tensor.scalar(name='sigma', dtype='float32')\n",
    "\n",
    "  embedding_batch_middle = lasagne.layers.batch_norm(\n",
    "      lasagne.layers.NonlinearityLayer( network,  nonlinearity=lasagne.nonlinearities.rectify )\n",
    "  )\n",
    "\n",
    "  embedding_batch_middle = lasagne.layers.GaussianNoiseLayer(\n",
    "            embedding_batch_middle, \n",
    "            sigma=0.2 * theano.tensor.exp((-0.01) * sigma ) # Noise should die down over time...\n",
    "  )  \n",
    "\n",
    "\n",
    "  sparsity_blend = theano.tensor.exp((-10.) * sigma )  # Goes from 1 to epsilon\n",
    "  current_sparsity = 0.50*(sparsity_blend) + args.sparsity*(1. - sparsity_blend)\n",
    "\n",
    "  sparse_embedding_batch_squashed = SparseWinnerTakeAllLayerApprox(\n",
    "                                      embedding_batch_middle, \n",
    "                                      approx_sparsity=current_sparsity\n",
    "                                    )\n",
    "    \n",
    "    \n",
    "elif args.mode == 'predict':\n",
    "  embedding_batch_middle = lasagne.layers.batch_norm(\n",
    "      lasagne.layers.NonlinearityLayer( network,  nonlinearity=lasagne.nonlinearities.rectify )\n",
    "    )\n",
    "        \n",
    "  #sparse_embedding_batch_squashed = SparseWinnerTakeAllLayer(\n",
    "  #                                    embedding_batch_middle, \n",
    "  #                                    sparsity=args.sparsity,\n",
    "  #                                  )\n",
    "\n",
    "  sparse_embedding_batch_squashed = SparseWinnerTakeAllLayerApprox(\n",
    "                                      embedding_batch_middle, \n",
    "                                      approx_sparsity=args.sparsity,   # Jam the actual (final) value in...\n",
    "                                    )\n",
    "    \n",
    "if sparse_embedding_batch_probs is None:\n",
    "  sparse_embedding_batch_probs = sparse_embedding_batch_squashed\n",
    "\n",
    "network = sparse_embedding_batch_squashed\n",
    "\n",
    "#if version==22:\n",
    "#  network = lasagne.layers.DenseLayer(\n",
    "#      network,\n",
    "#      num_units=embedding_dim*2,\n",
    "#      nonlinearity=lasagne.nonlinearities.rectify,\n",
    "#      W=lasagne.init.GlorotUniform(),\n",
    "#      b=lasagne.init.Constant(0.)\n",
    "#    )\n",
    "\n",
    "network = lasagne.layers.DenseLayer(\n",
    "    network,\n",
    "    num_units=embedding_dim,\n",
    "    nonlinearity=lasagne.nonlinearities.linear,\n",
    "    W=lasagne.init.GlorotUniform(),\n",
    "    b=lasagne.init.Constant(0.)\n",
    "  )\n",
    "\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "\n",
    "l2_error = lasagne.objectives.squared_error( prediction, embedding_batch )   \n",
    "l2_error_mean = l2_error.mean()  # This is a per-element error term\n",
    "\n",
    "interim_output = lasagne.layers.get_output(sparse_embedding_batch_probs)\n",
    "\n",
    "# Count the number of positive entries\n",
    "sparse_flag = theano.tensor.switch( theano.tensor.ge(interim_output, 0.0001), 1.0, 0.0)\n",
    "\n",
    "#sparsity_mean  = sparse_flag.mean() / args.sparsity  # This is a number 0..1, where 1.0 = perfect = on-target\n",
    "sparsity_mean  = sparse_flag.mean() * 100.  # This is realised sparsity \n",
    "\n",
    "sparsity_std  = (sparse_flag.mean(axis=1) / args.sparsity).std()     # assess the 'quality' of the sparsity per-row\n",
    "\n",
    "# This is to monitor learning (not direct it)\n",
    "sparsity_probe = sparse_flag.mean(axis=1) / args.sparsity # sparsity across rows may not be ===1.0\n",
    "#sparsity_probe = sparse_flag.mean(axis=0) / args.sparsity # sparsity across columns should be ===1.0 (if approximation works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparsity_cost=0.0\n",
    "if args.mode == 'train':\n",
    "  mix = theano.tensor.scalar(name='mix', dtype='float32')\n",
    "\n",
    "  sparsity_cost = -mix*sparsity_mean/1000.  # The 1000 factor is because '10' l2 is Ok, and 1 sparsity_mean is Great\n",
    "  if version==20 or version==21:\n",
    "    sparsity_cost = mix*0.\n",
    "  \n",
    "cost = l2_error_mean + sparsity_cost\n",
    "\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_base=0\n",
    "if args.load:\n",
    "  load_vars = hickle.load(args.load)\n",
    "  print(\"Saved file had : Epoch:%4d, sigma:%5.2f\" % (load_vars['epoch'], load_vars['sigma'], ) )\n",
    "  #fraction_of_vocab=fraction_of_vocab\n",
    "  \n",
    "  epoch_base = load_vars['epoch']\n",
    "  \n",
    "  if 'layer_names' in load_vars:\n",
    "    layer_names = load_vars['layer_names']\n",
    "  else:\n",
    "    i=0\n",
    "    layer_names=[]\n",
    "    while \"Lasagne%d\" % (i,) in load_vars:\n",
    "      layer_names.append( \"Lasagne%d\" % (i,) )\n",
    "      i=i+1\n",
    "    \n",
    "  layers = [ load_vars[ ln ] for ln in layer_names ]\n",
    "  \n",
    "  lasagne.layers.set_all_param_values(network, layers)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if args.mode == 'train':\n",
    "  updates = lasagne.updates.adam( cost, params )\n",
    "\n",
    "  #iterate_net = theano.function( [batch_start_index], [l2_error_mean,sparsity_mean], updates=updates, \n",
    "  iterate_net = theano.function( \n",
    "                  [batch_start_index,sigma,mix], \n",
    "                  [l2_error_mean,sparsity_mean,sparsity_std,sparsity_probe], \n",
    "                  updates=updates, \n",
    "                  allow_input_downcast=True,\n",
    "                  on_unused_input='warn',\n",
    "                )\n",
    "\n",
    "  print(\"Built Theano op graph\")\n",
    "  \n",
    "  sigma_ = 0.0\n",
    "  mix_ = 0.0\n",
    "  if args.param:\n",
    "    mix_=args.param\n",
    "  \n",
    "  t0 = time.time()\n",
    "  for epoch in range(epoch_base, epoch_base+args.iters):\n",
    "    t1 = time.time()\n",
    "    \n",
    "    if version<8:\n",
    "      fraction_of_vocab = 0.1 + epoch*(0.05)\n",
    "      if fraction_of_vocab>1.0: \n",
    "        fraction_of_vocab=1.0\n",
    "\n",
    "      if epoch>20:\n",
    "        if epoch % 10 == 0:\n",
    "          sigma_ += 0.02\n",
    "      \n",
    "      if epoch>1000:\n",
    "        sigma_ = 2.0\n",
    "    \n",
    "    if version>=8:\n",
    "      fraction_of_vocab = 1.0\n",
    "\n",
    "    max_l2_error_mean=-1000.0\n",
    "\n",
    "    batch_list = np.array( range(0, int(embedding.shape[0]*fraction_of_vocab), batchsize) )\n",
    "    batch_list = np.random.permutation( batch_list )\n",
    "    \n",
    "    for b_start in batch_list.astype(int).tolist():\n",
    "      #l2_error_mean_,sparsity_mean_ = iterate_net(b_start)\n",
    "      \n",
    "      l2_error_mean_,sparsity_mean_,sparsity_std_,sparsity_probe_ = iterate_net(b_start, sigma_, mix_)\n",
    "\n",
    "      print(\" epoch:,%4d, b:,%7d, l2:,%9.2f, sparsity_mean_:,%9.4f, sparsity_std_:,%9.4f, sigma:,%5.2f, mix:,%5.2f, \" % \n",
    "          (epoch, b_start, 1000*l2_error_mean_, sparsity_mean_, sparsity_std_, sigma_, mix_, ))\n",
    "\n",
    "      if b_start==0:\n",
    "        #print(\"Hurdles : \" + np_int_list( sparsity_probe_[0:100] ))\n",
    "        print(\"  Row-wise sparsity : \" + np_int_list( sparsity_probe_[0:30] ))\n",
    "        #print(\"  %d, vector_probe : %s\" % (epoch, np_int_list( np.sort(sparsity_probe_[0:100]) ), )) \n",
    "        #print(\"  %d, vector_probe : %s\" % (epoch, np_int_list( sparsity_probe_[0:100] ), )) \n",
    "        #print(\"  vector_probe : \" + np_int_list( sparsity_probe_[0:1000] ))\n",
    "      \n",
    "      if max_l2_error_mean<l2_error_mean_:\n",
    "        max_l2_error_mean=l2_error_mean_\n",
    "\n",
    "    print(\"Time per 100k words %6.2fs\" % ((time.time() - t1)/embedding.shape[0]/fraction_of_vocab*1000.*100.,  ))\n",
    "    #exit()\n",
    "\n",
    "    boil_limit=10.\n",
    "    if version==14:\n",
    "      boil_limit=5.\n",
    "    \n",
    "    if args.normalize:\n",
    "      boil_limit=40.\n",
    "    \n",
    "    if max_l2_error_mean*1000.<boil_limit and version<99:\n",
    "      print(\"max_l2_error_mean<%6.2f - increasing sparseness emphasis\" % (boil_limit,))\n",
    "      if version<11 and sigma_<2.0 :\n",
    "        sigma_ += 0.01\n",
    "      if version>=11:\n",
    "        sigma_ += 0.01\n",
    "      mix_ += 0.1\n",
    "\n",
    "    if (epoch +1) % 10 == 0:\n",
    "      save_vars = dict(\n",
    "        version=version,\n",
    "        epoch=epoch,\n",
    "        sigma=sigma_,\n",
    "        mix=mix_,\n",
    "        fraction_of_vocab=fraction_of_vocab\n",
    "      )\n",
    "\n",
    "      layer_names = []\n",
    "      for i,p in enumerate(lasagne.layers.get_all_param_values(network)):\n",
    "        if len(p)>0:\n",
    "          name = \"Lasagne%d\" % (i,)\n",
    "          save_vars[ name ] = p\n",
    "          layer_names.append( name )\n",
    "      save_vars[ 'layer_names' ] = layer_names\n",
    "    \n",
    "      #epoch_thinned = epoch\n",
    "      #epoch_thinned = int(epoch/10)*10\n",
    "      #epoch_thinned = int(epoch/50)*50\n",
    "      epoch_thinned = int(epoch/100)*100\n",
    "      hickle.dump(save_vars, args.save % (epoch_thinned,), mode='w', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if args.load and args.mode == 'predict':\n",
    "  print(\"Parameters : \", lasagne.layers.get_all_params(network))\n",
    "  \n",
    "  get_sparse_linear = theano.function( [batch_start_index], [ lasagne.layers.get_output(sparse_embedding_batch_linear), ])  # allow_input_downcast=True \n",
    "  predict_net = theano.function( [batch_start_index], [l2_error_mean,sparsity_mean], allow_input_downcast=True )\n",
    "  predict_emb = theano.function( [batch_start_index], [prediction], allow_input_downcast=True )\n",
    "\n",
    "  predict_bin = theano.function( [batch_start_index], [ lasagne.layers.get_output(sparse_embedding_batch_squashed),])\n",
    "\n",
    "  print(\"Built Theano op graph\")\n",
    "\n",
    "  if True:  # Shows the error predictions with hard01 sigmoid\n",
    "    for b_start in range(0, int(embedding.shape[0]), batchsize):\n",
    "      l2_error_mean_,sparsity_mean_ = predict_net(b_start)\n",
    "\n",
    "      print(\" epoch:%4d, b:%7d, l2:%12.4f, sparsity:%6.4f - hard01\" % \n",
    "          (epoch_base, b_start, 1000*l2_error_mean_, sparsity_mean_, ))\n",
    "\n",
    "  if False:  # Shows the linear range of the sparse layer (pre-squashing)\n",
    "    for b_start in range(0, int(embedding.shape[0]), batchsize * 5):\n",
    "      sparse_embedding_batch_linear_, = get_sparse_linear(b_start)\n",
    "\n",
    "      for row in range(0,100,5):\n",
    "        print(np_int_list( sparse_embedding_batch_linear_[row][0:1000:50], mult=10, size=4 ))\n",
    "\n",
    "  if args.output:\n",
    "    predictions=[]\n",
    "    for b_start in range(0, int(embedding.shape[0]), batchsize):\n",
    "      prediction_, = predict_emb(b_start)\n",
    "      \n",
    "      predictions.append( np.array( prediction_ ) )\n",
    "\n",
    "      print(\" epoch:%3d, b:%7d, Downloading - reconstructed array\" % \n",
    "          (epoch_base, b_start, ))\n",
    "    \n",
    "    embedding_prediction = np.concatenate(predictions, axis=0)\n",
    "    predictions=None\n",
    "\n",
    "    print(\"About to save to %s\" % (args.output,))\n",
    "    d=dict( \n",
    "      vocab=vocab, \n",
    "      vocab_orig=vocab_orig,\n",
    "      embedding=embedding_prediction,\n",
    "    )\n",
    "    hickle.dump(d, args.output, mode='w', compression='gzip')\n",
    "  \n",
    "  if args.direct:\n",
    "    predictions=[]\n",
    "    for b_start in range(0, int(embedding.shape[0]), batchsize):\n",
    "      binarised_, = predict_bin(b_start)\n",
    "      \n",
    "      #predictions.append( np.where( binarised_>0.5, 1., 0. ).astype('float32') )\n",
    "      predictions.append( binarised_.astype('float32') )\n",
    "\n",
    "      #print(\" epoch:%3d, b:%7d, Downloading - hard01 to binary\" % \n",
    "      print(\" epoch:%3d, b:%7d, Downloading - sparse data\" % \n",
    "          (epoch_base, b_start, ))\n",
    "    \n",
    "    embedding_prediction = np.concatenate(predictions, axis=0)\n",
    "    predictions=None\n",
    "\n",
    "    print(\"About to save sparse version to %s\" % (args.direct,))\n",
    "    d=dict( \n",
    "      vocab=vocab, \n",
    "      vocab_orig=vocab_orig,\n",
    "      embedding=embedding_prediction,\n",
    "    )\n",
    "    hickle.dump(d, args.direct, mode='w', compression='gzip')\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
