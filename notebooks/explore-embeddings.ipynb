{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressing Word Embeddings\n",
    "\n",
    "Downloadable version of GloVe embedding (with fallback source), as well as downloadable versions of sparsified GloVe embedding from own hosting.\n",
    "\n",
    "Include instructions for Levy test-suite installation, so that any given embedding can be tested.\n",
    "\n",
    "And functions/tools to play with the loaded embedding (of whatever type)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Pre-Built Embeddings\n",
    "\n",
    "The following needs to be Pythonized :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RCL_BASE=('http://redcatlabs.com/downloads/'+\n",
    "          'deep-learning-workshop/notebooks/data/'+\n",
    "          'research/ICONIP-2016/')\n",
    "\n",
    "\"\"\"\n",
    "# http://redcatlabs.com/downloads/deep-learning-workshop/LICENSE\n",
    "\n",
    "# Files in : ${RCL_BASE} :\n",
    "\n",
    "# :: These are either as downloaded from GloVe site, or generated by Levy code\n",
    "# 507206240 Oct 25  2015 2-pretrained-vectors_glove.6B.300d.hkl\n",
    "# 160569440 May 14 14:57 1-glove-1-billion-and-wiki_window11-lc-36_vectors.2-17.hkl\n",
    "\n",
    "# :: These are originals - citation desired...\n",
    "#  53984642 May 15 14:13 sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_4000_GPU-sparse_matrix.hkl\n",
    "# 148011260 May 15 14:13 sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_4000_GPU-sparsity_recreate.hkl\n",
    "\n",
    "#  57907373 May 15 02:53 sparse.6B.300d_S-21_2n-shuf-noise-after-norm_4k_.2.01_1-50_5000_GPU-sparse_matrix.hkl\n",
    "# 147946219 May 15 02:52 sparse.6B.300d_S-21_2n-shuf-noise-after-norm_4k_.2.01_1-50_5000_GPU-sparsity_recreate.hkl\n",
    "\n",
    "\n",
    "# Spare?\n",
    "# 122248980 May  2 13:09 misc/sparse.6B.300d_T-21_3500.1024@0.05-GPU-sparse_matrix.hkl\n",
    "# 447610336 May  2 13:04 misc/sparse.6B.300d_T-21_3500.1024@0.05-GPU-sparsity_recreate.hkl\n",
    "#  53312127 May 11 14:10 misc/sparse.6B.300d_S-21_2n-shuf_1024@6.75_2000_GPU-sparse_matrix.hkl\n",
    "# 148027055 May 11 14:10 misc/sparse.6B.300d_S-21_2n-shuf_1024@6.75_2000_GPU-sparsity_recreate.hkl\n",
    "#  57054795 May 11 12:09 misc/sparse.6B.300d_S-21_2n-shuf_4096@1.50_2000_GPU-sparse_matrix.hkl\n",
    "# 147997824 May 11 12:09 misc/sparse.6B.300d_S-21_2n-shuf_4096@1.50_2000_GPU-sparsity_recreate.hkl\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os, requests\n",
    "\n",
    "def get_embedding_file( hkl ):  \n",
    "    if not os.path.isfile(os.path.join('data', hkl)):\n",
    "        # ... requests.get( RCL_BASE + hkl)\n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Download the Omer-Levy Test Regime\n",
    "\n",
    "See : https://levyomer.files.wordpress.com/2015/03/improving-distributional-similarity-tacl-2015.pdf\n",
    "\n",
    "To download the test suite, please run the script ```download-tests.bash```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to test an Embedding .txt File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "from __future__ import print_function\n",
    "\n",
    "def test_embedding_file(vectors_txt, vocab_max=131072 ):\n",
    "  # Do we need to process VECTORS_FILE->{ VECTORS_WORDS, VECTORS_NPY }?\n",
    "  # Answer = YES : the .words is required, and is used to create .npy and .vocab\n",
    "    \n",
    "  vectors_txt_words = '%s.words' % (vectors_txt,)\n",
    "  if not os.path.isfile(vectors_txt_words) or os.stat(vectors_txt).st_mtime>os.stat(vectors_txt_words).st_mtime:\n",
    "    print(\"Reading %s\" % (vectors_txt,))\n",
    "    # This is just a copy of 'text file' with the vocab_size and embedding_size pre-pended\n",
    "    #echo \"131072 300\" > ${VECTORS_WORDS}\n",
    "    #head -131072 ${VECTORS_FILE} >> ${VECTORS_WORDS}\n",
    "    with open(vectors_txt) as fin:\n",
    "      first_line = fin.readline()\n",
    "      embedding_dim = len(first_line.strip().split()) -1 \n",
    "      vocab_size = len(fin.readlines()) +1  # Ouch! - read in whole file to find length\n",
    "\n",
    "      if vocab_size>vocab_max:\n",
    "        vocab_size=vocab_max\n",
    "            \n",
    "    print(\"Building %s\" % (vectors_txt_words,))\n",
    "    with open(vectors_txt) as fin:\n",
    "      with open(vectors_txt_words, 'wt') as fout:\n",
    "        # Write the first line, which, ironically, will be discarded by the omerlevy code\n",
    "        fout.write(\"%d %d\\n\" % (vocab_size, embedding_dim))\n",
    "                \n",
    "        # And copy over at most vocab_max lines of the original file \n",
    "        for i, line in enumerate(fin.readlines()):\n",
    "          if i>vocab_size:\n",
    "            break\n",
    "          fout.write(line)\n",
    "    print(\"Built %s as %d %d-d vectors\" % (vectors_txt_words, vocab_size, embedding_dim))\n",
    "    \n",
    "  vectors_txt_npy   = '%s.npy' % (vectors_txt_words,)\n",
    "  vectors_txt_vocab = '%s.vocab' % (vectors_txt_words,)\n",
    "  if not os.path.isfile(vectors_txt_npy) or os.stat(vectors_txt_words).st_mtime>os.stat(vectors_txt_npy).st_mtime:\n",
    "    print(\"Building %s and %s\" % (vectors_txt_npy, vectors_txt_vocab, ))\n",
    "    # Sadly, we can't just invoke this as a python function - need to go via shell...\n",
    "    subprocess.call([ \"python\", \"../omerlevy/hyperwords/text2numpy.py\", vectors_txt_words ])\n",
    "    print(\"Built %s and %s\" % (vectors_txt_npy, vectors_txt_vocab, ))\n",
    "\n",
    "   \n",
    "  def run_word_test(test_str, test_cmd):\n",
    "    print(\"  %s\" % ((test_str+' '*30)[:30],), end='')\n",
    "    #subprocess.call(test_cmd)\n",
    "    try:\n",
    "      res = subprocess.check_output( test_cmd, stderr=subprocess.STDOUT,)\n",
    "      print(\" : %s\" % (res.strip(),))\n",
    "    except subprocess.CalledProcessError as e:\n",
    "      print(\" : ERROR : %s\" % (test_str,))\n",
    "      print(e)\n",
    "\n",
    "  def run_word_similarity(test_str, test_set):\n",
    "    test_cmd = [ \n",
    "        \"python\", \"../omerlevy/hyperwords/ws_eval.py\", \"VECTORS\", vectors_txt, \n",
    "        \"../omerlevy/testsets/ws/%s\" % (test_set,) \n",
    "    ]\n",
    "    run_word_test(test_str, test_cmd)\n",
    "\n",
    "  def run_word_analogy(test_str, test_set):\n",
    "    #python ../omerlevy/hyperwords/analogy_eval.py VECTORS ${VECTORS_FILE} ../omerlevy/testsets/analogy/google.txt\n",
    "    test_cmd = [ \n",
    "        \"python\", \"../omerlevy/hyperwords/analogy_eval.py\", \"VECTORS\", vectors_txt, \n",
    "        \"../omerlevy/testsets/analogy/%s\" % (test_set,) \n",
    "    ]\n",
    "    run_word_test(test_str, test_cmd)\n",
    "\n",
    "  if True:\n",
    "    print(\"Word Similarity Tests (~5 seconds each)\")\n",
    "    run_word_similarity(\"WS353 Similarity  \", \"ws353_similarity.txt\")\n",
    "    run_word_similarity(\"WS353 Relatedness \", \"ws353_relatedness.txt\")\n",
    "    run_word_similarity(\"Bruni MEN         \", \"bruni_men.txt\")\n",
    "    run_word_similarity(\"Radinsky M.Turk   \", \"radinsky_mturk.txt\")\n",
    "    run_word_similarity(\"Luoung Rare Words \", \"luong_rare.txt\")\n",
    "\n",
    "  if True:\n",
    "    print(\"Word Analogy Tests (~60 seconds each)\")\n",
    "    run_word_analogy(\"Google Analogy    \", \"google.txt\")\n",
    "    run_word_analogy(\"MSR Analogy       \", \"msr.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose an embedding to Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#embedding_file = '../data/2-pretrained-vectors_glove.6B.300d.hkl'\n",
    "#embedding_file = '../data/1-glove-1-billion-and-wiki_window11-lc-36_vectors.2-17.hkl'\n",
    "\n",
    "#embedding_file = '../data/lloyds_normed_8.hkl'\n",
    "\n",
    "# 1024-d embeddings : sparse and recreated\n",
    "embedding_file = '../data/sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_4000_GPU-sparse_matrix.hkl'\n",
    "#embedding_file = '../data/sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_4000_GPU-sparsity_recreate.hkl'\n",
    "\n",
    "# 4096-d embeddings : sparse and recreated\n",
    "#embedding_file = '../data/sparse.6B.300d_S-21_2n-shuf-noise-after-norm_4k_.2.01_1-50_5000_GPU-sparse_matrix.hkl'\n",
    "#embedding_file = '../data/sparse.6B.300d_S-21_2n-shuf-noise-after-norm_4k_.2.01_1-50_5000_GPU-sparsity_recreate.hkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = hickle.load(embedding_file)\n",
    "vocab, embedding = d['vocab'], d['embedding']\n",
    "vocab_orig = d.get('vocab_orig', vocab)\n",
    "\n",
    "dictionary = dict( (word, i) for i,word in enumerate(vocab) if i<len(embedding) )\n",
    "dictionary_orig = dict( (word, i) for i,word in enumerate(vocab_orig) if i<len(embedding) )\n",
    "\n",
    "print(\"Embedding loaded :\", embedding.shape)   # (vocab_size, embedding_dimension)=(rows, columns)\n",
    "embedding_normed = embedding / np.linalg.norm(embedding, axis=1)[:, np.newaxis]\n",
    "\n",
    "def save_embedding_for_tests(vocab, embedding, save_filename_txt='../data/tmp.embedding.txt'):\n",
    "  with open(save_filename_txt, 'wb') as f:\n",
    "    for l in range(0, embedding.shape[0]):\n",
    "      f.write(\"%s %s\\n\" % (vocab[l], ' '.join([ ('0' if x==0. else (\"%.6f\" % (x,))) for x in embedding[l, :].tolist() ]), ))\n",
    "  print(\"Saved to %s\" % (save_filename_txt, ))\n",
    "\n",
    "def save_embedding_to_hickle(vocab, embedding_save, save_filename_hkl, vocab_orig=None):\n",
    "  print(\"About to save to %s\" % (save_filename_hkl,))\n",
    "  d=dict( \n",
    "    vocab=vocab, \n",
    "    vocab_orig=vocab if vocab_orig is None else vocab,\n",
    "    embedding=embedding_save,\n",
    "  )\n",
    "  hickle.dump(d, save_filename_hkl, mode='w', compression='gzip')\n",
    "  print(\"Saved to %s\" % (save_filename_hkl,))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab[0]\n",
    "entries = [ x for x in embedding[0].tolist() if x!=0.0 ]\n",
    "len(entries)\n",
    "#45 \n",
    "\n",
    "for w in 'the iraq baghdad uk london criminal apple some hypothesis maximal innocuous'.split(' '):\n",
    "  i=dictionary[w]\n",
    "  entries = [ x for x in embedding[i].tolist() if x!=0.0 ]\n",
    "  print(\"%20s @%6d len=%d\" % (w,dictionary_orig[w],len(entries),))\n",
    "\n",
    "  #               the @     0 len=18\n",
    "  #              some @    60 len=18\n",
    "  #            london @   266 len=91\n",
    "  #                uk @   448 len=82\n",
    "  #              iraq @   606 len=113\n",
    "  #          criminal @  1449 len=104\n",
    "  #             apple @  2046 len=112\n",
    "  #           baghdad @  2320 len=116\n",
    "  #        hypothesis @  6957 len=136\n",
    "  #           maximal @ 27962 len=107\n",
    "  #         innocuous @ 30111 len=86\n",
    "\n",
    "\n",
    "if False:\n",
    "  # Look at per-position best words\n",
    "  for i in range(0, embedding.shape[1], 10):\n",
    "    best_words_j = np.argsort( -embedding[:, i ] )[0:10]\n",
    "    for j in best_words_j:\n",
    "      print(\"%4i -> %s\" % (i, vocab[j],))\n",
    "    print('')\n",
    "\n",
    "if False:\n",
    "  #i=2000\n",
    "  values = [x for x in (-np.sort( -embedding[i] )).tolist() if x>0. ]\n",
    "  print(\"values: [\"+', '.join([ ('%.4f' % (x,)) for x in values ])+']')\n",
    "  #values: [1.1442, 0.9337, 0.9333, 0.9257, 0.7520, 0.5529, 0.4818, 0.4740, 0.4568, 0.4554, 0.4434, 0.4419, 0.4334, 0.4187, 0.4175, 0.4068, 0.4005, 0.3989, 0.3698, 0.3421, 0.3206, 0.3151, 0.3150, 0.3120, 0.3119, 0.3067, 0.3010, 0.2948, 0.2853, 0.2828, 0.2816, 0.2815, 0.2799, 0.2793, 0.2764, 0.2714, 0.2636, 0.2570, 0.2507, 0.2487, 0.2336, 0.2336, 0.2335, 0.2328, 0.2325, 0.2323, 0.2255, 0.2227, 0.2227, 0.2226, 0.2208, 0.2178, 0.2159, 0.2134, 0.2067, 0.2049, 0.1947, 0.1935, 0.1932, 0.1926, 0.1921, 0.1914, 0.1897, 0.1894, 0.1832, 0.1782, 0.1766, 0.1730, 0.1714, 0.1683, 0.1662, 0.1638, 0.1629, 0.1602, 0.1568, 0.1561, 0.1452, 0.1419, 0.1399, 0.1372, 0.1370, 0.1352, 0.1350, 0.1342, 0.1334, 0.1334, 0.1302, 0.1289, 0.1268, 0.1243, 0.1230, 0.1211, 0.1192, 0.1113, 0.1051]\n",
    "\n",
    "  print(\"changes: [\"+', '.join([ ('%.1f' % (values[i+1]/values[i]*100.,)) for i in range(0,len(values)-1) ])+']')\n",
    "  #changes: [81.6, 100.0, 99.2, 81.2, 73.5, 87.1, 98.4, 96.4, 99.7, 97.3, 99.7, 98.1, 96.6, 99.7, 97.4, 98.4, 99.6, 92.7, 92.5, 93.7, 98.3, 100.0, 99.0, 100.0, 98.3, 98.1, 97.9, 96.8, 99.1, 99.6, 100.0, 99.4, 99.8, 99.0, 98.2, 97.1, 97.5, 97.6, 99.2, 93.9, 100.0, 100.0, 99.7, 99.9, 99.9, 97.1, 98.8, 100.0, 100.0, 99.2, 98.6, 99.1, 98.9, 96.9, 99.1, 95.0, 99.4, 99.9, 99.7, 99.8, 99.6, 99.1, 99.8, 96.7, 97.3, 99.1, 98.0, 99.1, 98.2, 98.8, 98.6, 99.4, 98.3, 97.9, 99.5, 93.1, 97.7, 98.6, 98.1, 99.8, 98.7, 99.9, 99.4, 99.4, 100.0, 97.6, 99.0, 98.4, 98.0, 99.0, 98.4, 98.5, 93.4, 94.4]\n",
    "\n",
    "\n",
    "w='motorcycle'\n",
    "w_i=dictionary[w]\n",
    "\n",
    "#top_i =np.argmax(embedding[w_i])\n",
    "good_i =np.argsort( -embedding[w_i] )\n",
    "\n",
    "for i in range(0,10):\n",
    "  best_words_j = np.argsort( -embedding[:, good_i[i] ] )[0:12]\n",
    "  \n",
    "  #for j in best_words_j:\n",
    "  #  print(\"%s\" % (vocab[j],))\n",
    "  #print('')\n",
    "  \n",
    "  print(\"%s\" % (', '.join( [ vocab[j] for j in best_words_j] ), ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Embedding Exploration Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_for(w):\n",
    "  w_i=dictionary[w]\n",
    "  return embedding[w_i]\n",
    "\n",
    "def l2_normed(e):\n",
    "  return e / np.sqrt( np.dot(e,e) )\n",
    "\n",
    "def cosine(a,b):\n",
    "  return np.dot(l2_normed(a), l2_normed(b))\n",
    "\n",
    "def top_senses_for(e):\n",
    "  good_i = np.argsort( -e )\n",
    "  for i in range(0,10):\n",
    "    best_words_j = np.argsort( -embedding[:, good_i[i] ] )[0:12]\n",
    "    print(\"%s\" % (', '.join( [ vocab[j] for j in best_words_j] ), ) )\n",
    "\n",
    "def closest_to(e, n=10):\n",
    "  closest = np.argsort( - np.dot(embedding_normed, l2_normed(e) ) )\n",
    "  return \"%s\" % (', '.join( [ vocab[j] for j in closest[0:n] ] ), ) \n",
    "\n",
    "def count_positive(e):\n",
    "  return len( [ x for x in e.tolist() if x>0.0 ] )\n",
    "\n",
    "def nonzero_positions(e):\n",
    "    return [ i for (i,x) in enumerate(e.tolist()) if x!=0.0 ]\n",
    "\n",
    "def nonneg(e):\n",
    "  return np.maximum(0, e)\n",
    "\n",
    "def closest_dist(s):\n",
    "  ab,xy = s.split('=')\n",
    "  (a,b),(x,y) = ab.split(':'), xy.split(':')\n",
    "  print( \"%s is to %s as %s is to ?%s? \" % (a,b,x,y,))\n",
    "  (a,b,x,y) = map(vector_for, [a,b,x,y])  # Convert to vectors\n",
    "  print('  x+b-a           = %s' % (closest_to( x + b - a ),))\n",
    "  print('  [x+b-a]         = %s' % (closest_to( nonneg(x + b - a) ),))\n",
    "  print('  x+[b-a]         = %s' % (closest_to( x + nonneg(b-a) ),))\n",
    "  print('  [x-a]+b         = %s' % (closest_to( nonneg(x-a) + b ),))\n",
    "  print('  [2x-a]+[2b-a]   = %s' % (closest_to( nonneg(2*x-a) + nonneg(2*b-a) ),))\n",
    "  print('  x+[b-a]+b+[x-a] = %s' % (closest_to( x+nonneg(b-a) + b+nonneg(x-a) ),))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_senses_for(vector_for('motorbike'))\n",
    "\n",
    "man   = vector_for('man')\n",
    "woman = vector_for('woman')\n",
    "king  = vector_for('king')\n",
    "queen = vector_for('queen')\n",
    "\n",
    "#top_senses_for(man)\n",
    "#top_senses_for(woman)\n",
    "#top_senses_for(king)\n",
    "#top_senses_for(queen)\n",
    "\n",
    "#top_senses_for(man * woman) # Intersection\n",
    "#top_senses_for(man + woman) # Union\n",
    "#top_senses_for(man - woman) # ??\n",
    "\n",
    "\n",
    "closest_to(man)\n",
    "#man, woman, girl, person, men, teenager, she, friend, he, father, her, boy, someone, mother, him, his, victim, son, who, guy\n",
    "closest_to(woman)\n",
    "#woman, man, girl, mother, teenager, daughter, wife, women, her, person, she, girlfriend, friend, men, husband, widow, couple, boy, someone, victim\n",
    "\n",
    "closest_to(king)\n",
    "#king, queen, henry, mswati, mongkut, eirik, charles, vajiravudh, thoden, wenceslaus, zvonimir, athelstan, vladislaus, thelred, gojong, prince, jayavarman, kalkaua, sweyn, pomare\n",
    "closest_to(queen)\n",
    "#queen, princess, elizabeth, king, margrethe, empress, lady, sister, prince, sirikit, mary, cixi, monarch, daughter, duchess, olten, mother, infanta, rania, widow\n",
    "\n",
    "closest_dist('pound:england=franc:france')\n",
    "\n",
    "\n",
    "england,pound,america,dollar = map(vector_for, 'england pound america dollar'.split())\n",
    "\n",
    "curr = england,pound,america,dollar = map(vector_for, 'england pound america dollar'.split())\n",
    "map(count_positive, curr)\n",
    "#[84, 126, 94, 134]\n",
    "\n",
    "map(count_positive, [ england*pound, america*dollar, england*america, pound*dollar])\n",
    "#[12, 14, 17, 56]\n",
    "\n",
    "total = england+pound+america+dollar\n",
    "map(count_positive, [ england*101-100*total, pound*101-100*total, america*101-100*total, dollar*101-100*total])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_embedding_for_tests(vocab, embedding, save_filename_txt='../data/tmp.embedding.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_embedding_file('../data/tmp.embedding.txt', vocab_max=131072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save_embedding_for_tests(vocab, embedding, save_filename_txt='../data/lloyds_normed_8.txt')\n",
    "#test_embedding_file('../data/lloyds_normed_8.txt', vocab_max=131072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(nonzero_positions(embedding[10202]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's make a *random* transformation matrix to 'recreate' and approximate 300d dense embedding\n",
    "A = np.random.normal(loc=0.0, scale=1.0, size=(embedding.shape[1], 300))\n",
    "\n",
    "embedding_reconstructed = np.dot(embedding, A)        # Project back down to 300d using random matrix\n",
    "embedding_reconstructed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_reconstructed = embedding/5.0               # Check that scaling is irrelevant (ACTUALLY, THIS CHECK IS BOGUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_reconstructed = np.where(embedding>0,1,0)   # Binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_reconstructed = np.where(embedding>0,0,1)   # Binarized (inverted) is TERRIBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_d(a_i, b_i, c_i, emb):\n",
    "  a = emb[a_i]\n",
    "  b = emb[b_i]\n",
    "  c = emb[c_i]\n",
    "  d = c+b-a              # This is the standard np() vector constructor\n",
    "  return np.dot(emb, d.T)   # Return the score of this vs all the embedding vectors\n",
    "\n",
    "# Let's build an analogy tester right here...\n",
    "def test_analogies(test_set=\"msr\", emb=embedding, construction_fn=construct_d):\n",
    "  trials, trials_w,total,total_possible=[],[],0,0\n",
    "  with open(\"../omerlevy/testsets/analogy/%s.txt\" % test_set) as f:  # 'google' or 'msr'\n",
    "    for trial in f.readlines():\n",
    "      t_w = [ w for w in trial.strip().split() ]\n",
    "      t = [ dictionary.get(w, None) for w in t_w ]\n",
    "      total+=1\n",
    "      if None not in t:\n",
    "        trials_w.append(t_w)\n",
    "        trials.append(t)\n",
    "        total_possible+=1\n",
    "    \n",
    "  print(trials_w[0], trials[0])\n",
    "\n",
    "  count, correct = 0, 0\n",
    "  for i,trial in enumerate(trials):\n",
    "    if i % 10 >0:  # 10x thinning factor for speed (1 for accuracy)\n",
    "      continue\n",
    "    d_score = construction_fn( trial[0], trial[1], trial[2], emb)\n",
    "    \n",
    "    # Set the scores for the original vectors to useless values\n",
    "    d_score[ trial[0] ] = d_score[ trial[1] ] = d_score[ trial[2] ] = -1\n",
    "    \n",
    "    # Now find the argmax score:\n",
    "    d_i = np.argmax( d_score )\n",
    "    #print( d_score.shape, d_i, i )\n",
    "    \n",
    "    if True and False:\n",
    "      w = trials_w[i]\n",
    "      print(\"'%s:%s=%s:%s'?  :: %s\" % ( w[0], w[1], w[2], vocab[ d_i ],\n",
    "                ('WIN' if trial[3]==d_i else \"FAIL\"),\n",
    "            ))\n",
    "    \n",
    "    count += 1\n",
    "    if trial[3]==d_i:\n",
    "      correct += 1\n",
    "    \n",
    "    if i % 250==0:\n",
    "      print(\"At %d : %.2f%%\" % (count, 100.*correct/count))\n",
    "  print(\"Local(%s) final : %.2f%% using %d, which is %.2f%% of total\" % (test_set, \n",
    "    100.*correct/count, count, 100.*correct/count*total_possible/total,))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_analogies(emb=embedding_normed) # , test_set=\"google\")\n",
    "#NO test_analogies(emb=embedding) # , test_set=\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remax(a):\n",
    "    return a / np.amax(a)\n",
    "\n",
    "def a_with_b(a, b):\n",
    "    return remax(a+b)\n",
    "\n",
    "def a_without_b(a, b):\n",
    "    c = a.copy()\n",
    "    c[ b>0 ]=0.\n",
    "    return remax(c)\n",
    "    \n",
    "def closest_dist(s):\n",
    "  ab,xy = s.split('=')\n",
    "  (a,b),(x,y) = ab.split(':'), xy.split(':')\n",
    "  print( \"%s is to %s as %s is to ?%s? \" % (a,b,x,y,))\n",
    "  (a,b,x,y) = map(vector_for, [a,b,x,y])  # Convert to vectors\n",
    "  print('  x+b-a           = %s' % (closest_to( x + b - a ),))\n",
    "  print('  [x+b-a]         = %s' % (closest_to( nonneg(x + b - a) ),))\n",
    "  print('  x+[b-a]         = %s' % (closest_to( x + nonneg(b-a) ),))\n",
    "  print('  [x-a]+b         = %s' % (closest_to( nonneg(x-a) + b ),))\n",
    "  print('  [2x-a]+[2b-a]   = %s' % (closest_to( nonneg(2*x-a) + nonneg(2*b-a) ),))\n",
    "  print('  x+[b-a]+b+[x-a] = %s' % (closest_to( x+nonneg(b-a) + b+nonneg(x-a) ),))\n",
    "  setish1 = a_with_b( a_without_b( x,  a_without_b(a, b) ), a_without_b(b, a))\n",
    "  print('  setish1         = %s' % (closest_to( setish1 ),))\n",
    "  setish2 = a_with_b( a_without_b( b,  a_without_b(a, x) ), a_without_b(x, a))\n",
    "  print('  setish2         = %s' % (closest_to( setish2 ),))\n",
    "\n",
    "#closest_dist('pound:england=franc:france')\n",
    "closest_dist('london:england=paris:france')\n",
    "closest_dist('smallest:smaller=smoothest:smoother')  # Better with set ops\n",
    "#closest_dist('great:greater=classy:classier')  # No idea\n",
    "#closest_dist('richest:richer=meanest:meaner')  # No idea\n",
    "#closest_dist('seem:seems=develop:develops')\n",
    "closest_dist('few:fewer=friendly:friendlier')\n",
    "\n",
    "#### All MSR sets with 'fast' in them are bogus\n",
    "##closest_dist('weak:weakest=fastest:fast') !!!\n",
    "\n",
    "e=vector_for('england')\n",
    "l=vector_for('london')\n",
    "f=vector_for('france')\n",
    "p=vector_for('paris')\n",
    "#closest_to( p+e-l )\n",
    "\n",
    "p_poss1 = a_with_b( a_without_b( p,  a_without_b(l, e) ), a_without_b(e, l))\n",
    "p_poss2 = a_with_b( a_without_b( e,  a_without_b(l, p) ), a_without_b(p, l))\n",
    "print( closest_to( p_poss1 ) )\n",
    "print( closest_to( p_poss2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construct_d_setish(a_i, b_i, c_i, emb):\n",
    "  a = emb[a_i]\n",
    "  b = emb[b_i]\n",
    "  c = emb[c_i]\n",
    "  #d = c + (b-a)              # This is the standard np() vector constructor\n",
    "  #d = a_with_b( a_without_b( c,  a_without_b(a, b) ), a_without_b(b, a))  # This is set-ish  v1\n",
    "  #d = a_with_b( a_without_b( b,  a_without_b(a, c) ), a_without_b(c, a))  # This is set-ish  v2\n",
    "  d = a_with_b(a_with_b( c,  a_without_b(b, a) ), a_with_b( b, a_without_b(c, a)))  # This is set-ish  v3\n",
    "  d += c + (b-a)\n",
    "  return np.dot(emb, d.T)   # Return the score of this vs all the embedding vectors\n",
    "\n",
    "test_analogies(emb=embedding_normed, construction_fn=construct_d_setish, test_set=\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# =0,=1, A/B,A+B,B/A, A/C,A+C,C/A \n",
    "a = np.array( [0.,1., 1.,1.,0., 1.,1.,0.])\n",
    "b = np.array( [0.,1., 0.,1.,1., 0.,0.,0.])\n",
    "c = np.array( [0.,1., 0.,0.,0., 0.,1.,1.])\n",
    "d = np.array( [0.,1., 0.,0.,1., 0.,0.,1.]) #?\n",
    "d0 = c + (b-a)\n",
    "d1 = a_with_b( a_without_b( c,  a_without_b(a, b) ), a_without_b(b, a))  # This is set-ish  v1\n",
    "d2 = a_with_b( a_without_b( b,  a_without_b(a, c) ), a_without_b(c, a))  # This is set-ish  v2\n",
    "d3 = a_with_b(a_with_b( c,  a_without_b(b, a) ), a_with_b( b, a_without_b(c, a)))  # This is set-ish  v3\n",
    "d0,d3, d1,d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_reconstructed_normed = embedding_reconstructed / np.linalg.norm(embedding_reconstructed, axis=1)[:, np.newaxis]\n",
    "save_embedding_for_tests(vocab, embedding_reconstructed_normed, save_filename_txt='../data/tmp.embedding.txt')\n",
    "test_embedding_file('../data/tmp.embedding.txt', vocab_max=131072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab[ 0:10 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding[0, 0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_reconstructed[0, 0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
