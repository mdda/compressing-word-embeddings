
GloVe
  test_embedding_file('../data/tmp.embedding.txt', vocab_max=131072)
  Word Similarity Tests (~5 seconds each)
    WS353 Similarity               : VECTORS ../data/tmp.embedding.txt 	0.664
    WS353 Relatedness              : VECTORS ../data/tmp.embedding.txt 	0.573
    Bruni MEN                      : VECTORS ../data/tmp.embedding.txt 	0.749
    Radinsky M.Turk                : VECTORS ../data/tmp.embedding.txt 	0.632
    Luoung Rare Words              : VECTORS ../data/tmp.embedding.txt 	0.268
  Word Analogy Tests (~60 seconds each)
    Google Analogy                 : VECTORS ../data/tmp.embedding.txt 	0.713 	0.724
    MSR Analogy                    : VECTORS ../data/tmp.embedding.txt 	0.554 	0.571

LLoyds  
  test_embedding_file('../data/tmp.embedding.txt', vocab_max=131072)
  Reading ../data/lloyds_normed_8.txt
  Building ../data/lloyds_normed_8.txt.words
  Built ../data/lloyds_normed_8.txt.words as 131072 300-d vectors
  Built ../data/lloyds_normed_8.txt.words.npy and ../data/lloyds_normed_8.txt.words.vocab
  Word Similarity Tests (~5 seconds each)
    WS353 Similarity               : VECTORS ../data/lloyds_normed_8.txt 	0.669
    WS353 Relatedness              : VECTORS ../data/lloyds_normed_8.txt 	0.584
    Bruni MEN                      : VECTORS ../data/lloyds_normed_8.txt 	0.752
    Radinsky M.Turk                : VECTORS ../data/lloyds_normed_8.txt 	0.646
    Luoung Rare Words              : VECTORS ../data/lloyds_normed_8.txt 	0.271
  Word Analogy Tests (~60 seconds each)
    Google Analogy                 : VECTORS ../data/lloyds_normed_8.txt 	0.706 	0.710
    MSR Analogy                    : VECTORS ../data/lloyds_normed_8.txt 	0.539 	0.555


**** MOST INTERESTING **** (See below)
sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_4000_GPU-sparse_matrix.hkl
  Reading ../data/tmp.embedding.txt
  Building ../data/tmp.embedding.txt.words
  Built ../data/tmp.embedding.txt.words as 131072 1024-d vectors
  Built ../data/tmp.embedding.txt.words.npy and ../data/tmp.embedding.txt.words.vocab
  Word Similarity Tests (~5 seconds each)
    WS353 Similarity               : VECTORS ../data/tmp.embedding.txt 	0.692
    WS353 Relatedness              : VECTORS ../data/tmp.embedding.txt 	0.513
    Bruni MEN                      : VECTORS ../data/tmp.embedding.txt 	0.695
    Radinsky M.Turk                : VECTORS ../data/tmp.embedding.txt 	0.621
    Luoung Rare Words              : VECTORS ../data/tmp.embedding.txt 	0.174
  Word Analogy Tests (~60 seconds each)
    Google Analogy                 : VECTORS ../data/tmp.embedding.txt 	0.525 	0.551
    MSR Analogy                    : VECTORS ../data/tmp.embedding.txt 	0.405 	0.438  

sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_4000_GPU-sparsity_recreate.hkl
  Reading ../data/tmp.embedding.txt
  Building ../data/tmp.embedding.txt.words
  Built ../data/tmp.embedding.txt.words as 131072 300-d vectors
  Built ../data/tmp.embedding.txt.words.npy and ../data/tmp.embedding.txt.words.vocab
  Word Similarity Tests (~5 seconds each)
    WS353 Similarity               : VECTORS ../data/tmp.embedding.txt 	0.647
    WS353 Relatedness              : VECTORS ../data/tmp.embedding.txt 	0.510
    Bruni MEN                      : VECTORS ../data/tmp.embedding.txt 	0.677
    Radinsky M.Turk                : VECTORS ../data/tmp.embedding.txt 	0.627
    Luoung Rare Words              : VECTORS ../data/tmp.embedding.txt 	0.219
  Word Analogy Tests (~60 seconds each)
    Google Analogy                 : VECTORS ../data/tmp.embedding.txt 	0.624 	0.629
    MSR Analogy                    : VECTORS ../data/tmp.embedding.txt 	0.490 	0.501



sparse.6B.300d_S-21_2n-shuf-noise-after-norm_4k_.2.01_1-50_5000_GPU-sparse_matrix.hkl  (MEMORY HEAVY : Needs>8Gb RAM)
  Built ../data/tmp.embedding.txt.words.npy and ../data/tmp.embedding.txt.words.vocab
  Word Similarity Tests (~5 seconds each)
    WS353 Similarity               : VECTORS ../data/tmp.embedding.txt 	0.637
    WS353 Relatedness              : VECTORS ../data/tmp.embedding.txt 	0.452
    Bruni MEN                      : VECTORS ../data/tmp.embedding.txt 	0.624
    Radinsky M.Turk                : VECTORS ../data/tmp.embedding.txt 	0.495
    Luoung Rare Words              : VECTORS ../data/tmp.embedding.txt 	0.180
  Word Analogy Tests (~60 seconds each)
    Google Analogy                 : VECTORS ../data/tmp.embedding.txt 	0.376 	0.408
    MSR Analogy                    : VECTORS ../data/tmp.embedding.txt 	0.273 	0.299


sparse.6B.300d_S-21_2n-shuf-noise-after-norm_4k_.2.01_1-50_5000_GPU-sparsity_recreate.hkl
  Reading ../data/tmp.embedding.txt
  Building ../data/tmp.embedding.txt.words
  Built ../data/tmp.embedding.txt.words as 131072 300-d vectors
  Built ../data/tmp.embedding.txt.words.npy and ../data/tmp.embedding.txt.words.vocab
  Word Similarity Tests (~5 seconds each)
    WS353 Similarity               : VECTORS ../data/tmp.embedding.txt 	0.650
    WS353 Relatedness              : VECTORS ../data/tmp.embedding.txt 	0.502
    Bruni MEN                      : VECTORS ../data/tmp.embedding.txt 	0.685
    Radinsky M.Turk                : VECTORS ../data/tmp.embedding.txt 	0.627
    Luoung Rare Words              : VECTORS ../data/tmp.embedding.txt 	0.222
  Word Analogy Tests (~60 seconds each)
    Google Analogy                 : VECTORS ../data/tmp.embedding.txt 	0.652 	0.664
    MSR Analogy                    : VECTORS ../data/tmp.embedding.txt 	0.518 	0.541










andrewsm@anson:/mnt/data/home/andrewsm/sketchpad/redcatlabs/embeddings/src/4-sparse/*.hkl ...


These are the 'standard embeddings'
 wget http://redcatlabs.com/downloads/deep-learning-workshop/notebooks/data/research/ICONIP-2016/2-pretrained-vectors_glove.6B.300d.hkl
 wget http://redcatlabs.com/downloads/deep-learning-workshop/notebooks/data/research/ICONIP-2016/1-glove-1-billion-and-wiki_window11-lc-36_vectors.2-17.hkl

These are the ICONIP-2016 'paper results'
 rsync -avz --progress andrewsm@anson:/mnt/data/home/andrewsm/sketchpad/redcatlabs/embeddings/src/4-sparse/sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_4000_GPU-spars* .
 rsync -avz --progress andrewsm@anson:/mnt/data/home/andrewsm/sketchpad/redcatlabs/embeddings/src/4-sparse/sparse.6B.300d_S-21_2n-shuf-noise-after-norm_4k_.2.01_1-50_5000_GPU-spars* .


cd ./data
  EMBEDDINGS=/home/andrewsm/sketchpad/redcatlabs/embeddings
  
  # Out-of-tree pre-built
  ln -s ${EMBEDDINGS}/data/1-glove-1-billion-and-wiki/window11-lc-36/vectors.2-17.hkl 1-glove-1-billion-and-wiki_window11-lc-36_vectors.2-17.hkl
  ln -s ${EMBEDDINGS}/data/2-pretrained-vectors/glove.6B.300d.hkl 2-pretrained-vectors_glove.6B.300d.hkl

  # All-in-one  pre-built
  ln -s ${EMBEDDINGS}/src/4-sparse/1-glove-1-billion-and-wiki_window11-lc-36_vectors.2-17.hkl .
  ln -s ${EMBEDDINGS}/src/4-sparse/2-pretrained-vectors_glove.6B.300d.hkl .

  # Generated 
  ln -s ${EMBEDDINGS}/src/4-sparse/sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_4000_GPU-sparse_matrix.hkl .
  ln -s ${EMBEDDINGS}/src/4-sparse/sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_4000_GPU-sparsity_recreate.hkl .
  ln -s ${EMBEDDINGS}/src/4-sparse/sparse.6B.300d_S-21_2n-shuf-noise-after-norm_4k_.2.01_1-50_5000_GPU-sparse_matrix.hkl .
  ln -s ${EMBEDDINGS}/src/4-sparse/sparse.6B.300d_S-21_2n-shuf-noise-after-norm_4k_.2.01_1-50_5000_GPU-sparsity_recreate.hkl .




GloVe
  test_embedding_file('../data/tmp.embedding.txt', vocab_max=131072)
  Word Similarity Tests (~5 seconds each)
    WS353 Similarity               : VECTORS ../data/tmp.embedding.txt 	0.664
    WS353 Relatedness              : VECTORS ../data/tmp.embedding.txt 	0.573
    Bruni MEN                      : VECTORS ../data/tmp.embedding.txt 	0.749
    Radinsky M.Turk                : VECTORS ../data/tmp.embedding.txt 	0.632
    Luoung Rare Words              : VECTORS ../data/tmp.embedding.txt 	0.268
  Word Analogy Tests (~60 seconds each)
    Google Analogy                 : VECTORS ../data/tmp.embedding.txt 	0.713 	0.724
    MSR Analogy                    : VECTORS ../data/tmp.embedding.txt 	0.554 	0.571

sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_4000_GPU-sparsity_recreate.hkl
  Word Similarity Tests (~5 seconds each)
    WS353 Similarity               : VECTORS ../data/tmp.embedding.txt 	0.647
    WS353 Relatedness              : VECTORS ../data/tmp.embedding.txt 	0.510
    Bruni MEN                      : VECTORS ../data/tmp.embedding.txt 	0.677
    Radinsky M.Turk                : VECTORS ../data/tmp.embedding.txt 	0.627
    Luoung Rare Words              : VECTORS ../data/tmp.embedding.txt 	0.219
  Word Analogy Tests (~60 seconds each)
    Google Analogy                 : VECTORS ../data/tmp.embedding.txt 	0.624 	0.629
    MSR Analogy                    : VECTORS ../data/tmp.embedding.txt 	0.490 	0.501




**** MOST INTERESTING ****
sparse.6B.300d_S-21_2n-shuf-noise-after-norm_.2.01_6-75_4000_GPU-sparse_matrix.hkl
  Reading ../data/tmp.embedding.txt
  Building ../data/tmp.embedding.txt.words
  Built ../data/tmp.embedding.txt.words as 131072 1024-d vectors
  Built ../data/tmp.embedding.txt.words.npy and ../data/tmp.embedding.txt.words.vocab
  Word Similarity Tests (~5 seconds each)
    WS353 Similarity               : VECTORS ../data/tmp.embedding.txt 	0.692
    WS353 Relatedness              : VECTORS ../data/tmp.embedding.txt 	0.513
    Bruni MEN                      : VECTORS ../data/tmp.embedding.txt 	0.695
    Radinsky M.Turk                : VECTORS ../data/tmp.embedding.txt 	0.621
    Luoung Rare Words              : VECTORS ../data/tmp.embedding.txt 	0.174
  Word Analogy Tests (~60 seconds each)
    Google Analogy                 : VECTORS ../data/tmp.embedding.txt 	0.525 	0.551
    MSR Analogy                    : VECTORS ../data/tmp.embedding.txt 	0.405 	0.438  
  Local(google) final : 51.30% using 1930, which is 50.64% of total     (approx 0.525?)
  Local(msr) final    : 48.58% using 669, which is 40.59% of total      (approx 0.405)
  Incorrectly using non-normed embedding : 
    Local(msr) final  : 38.57% using 669, which is 32.22% of total

  settish1 
    Local(msr) final : 45.89% using 669, which is 38.34% of total       (not better, not much worse)
  settish2
    Local(msr) final : 47.53% using 669, which is 39.71% of total       (not better, not much worse)
  settish3
    Local(msr) final : 49.18% using 669, which is 41.09% of total       (a tiny bit better)
  
embedding and embedding_normed are identical : So the tests implicitly norm...

GloVe * random(300, 300) -> embedding_reconstructed_randomly :: is actually slightly better that the original!

sparse_embedding * random(1024, 300) -> embedding_reconstructed_randomly  (embedding gets explicitly renormalized using Levy test suite)
  Word Similarity Tests (~5 seconds each)
    WS353 Similarity               : VECTORS ../data/tmp.embedding.txt 	0.597
    WS353 Relatedness              : VECTORS ../data/tmp.embedding.txt 	0.422
    Bruni MEN                      : VECTORS ../data/tmp.embedding.txt 	0.607
    Radinsky M.Turk                : VECTORS ../data/tmp.embedding.txt 	0.546
    Luoung Rare Words              : VECTORS ../data/tmp.embedding.txt 	0.182
  Word Analogy Tests (~60 seconds each)
    Google Analogy                 : VECTORS ../data/tmp.embedding.txt 	0.366 	0.348
    MSR Analogy                    : VECTORS ../data/tmp.embedding.txt 	0.294 	0.283
    
sparse_embedding -> binarized if np.where(embedding>0, 1, 0)                (embedding gets explicitly renormalized using Levy test suite)
  Word Similarity Tests (~5 seconds each)
    WS353 Similarity               : VECTORS ../data/tmp.embedding.txt 	0.620
    WS353 Relatedness              : VECTORS ../data/tmp.embedding.txt 	0.438
    Bruni MEN                      : VECTORS ../data/tmp.embedding.txt 	0.564
    Radinsky M.Turk                : VECTORS ../data/tmp.embedding.txt 	0.559
    Luoung Rare Words              : VECTORS ../data/tmp.embedding.txt 	0.146
  Word Analogy Tests (~60 seconds each)
    Google Analogy                 : VECTORS ../data/tmp.embedding.txt 	0.344 	0.347
    MSR Analogy                    : VECTORS ../data/tmp.embedding.txt 	0.265 	0.273



highest : higher == saddest : ?'strangest'  :: FAIL
high : highest == strict : ?'stricter'  :: FAIL
great : greater == classy : ?'grungy'  :: FAIL
great : greatest == fat : ?'fats'  :: FAIL
great : greater == costly : ?'expensive'  :: FAIL
lowest : lower == toughest : ?'tougher'  :: WIN
low : lowest == short : ?'longest'  :: FAIL
low : lower == old : ?'older'  :: WIN
largest : larger == richest : ?'wealthiest'  :: FAIL
large : largest == pretty : ?'thing'  :: FAIL
large : larger == lucky : ?'fortunate'  :: FAIL
oldest : older == lightest : ?'lighter'  :: WIN
old : oldest == few : ?'several'  :: FAIL
old : older == tall : ?'taller'  :: WIN
smallest : smaller == smoothest : ?'larger'  :: FAIL
small : smallest == broad : ?'broadest'  :: WIN
smallest : smaller == greatest : ?'larger'  :: FAIL
easy : easiest == smooth : ?'smoothest'  :: WIN
easy : easier == lean : ?'faster'  :: FAIL
early : earliest == narrow : ?'widest'  :: FAIL
early : earlier == noisy : ?'boisterous'  :: FAIL
earliest : earlier == deepest : ?'last'  :: FAIL
At 26 : 30.77%
young : youngest == quick : ?'boldest'  :: FAIL
young : younger == deep : ?'deeper'  :: WIN
youngest : younger == closest : ?'older'  :: FAIL
long : longest == kind : ?'sort'  :: FAIL
long : longer == mighty : ?'hardly'  :: FAIL
long : longest == smart : ?'nicest'  :: FAIL
bad : worse == weak : ?'weaker'  :: WIN
worst : worse == friendliest : ?'luckier'  :: FAIL
bad : worst == young : ?'youngest'  :: WIN
few : fewer == friendly : ?'decorous'  :: FAIL
fewest : fewer == cleanest : ?'than'  :: FAIL
few : fewest == free : ?'unrestricted'  :: FAIL
big : bigger == funny : ?'funnier'  :: WIN
biggest : bigger == nicest : ?'nicer'  :: WIN
big : biggest == bold : ?'largest'  :: FAIL
strong : stronger == messy : ?'livelier'  :: FAIL
strong : strongest == hard : ?'easiest'  :: FAIL
close : closer == early : ?'late'  :: FAIL
closest : closer == saddest : ?'strangest'  :: FAIL
close : closest == quick : ?'swift'  :: FAIL
broad : broader == cheap : ?'cheaper'  :: WIN
broadest : broader == highest : ?'increasing'  :: FAIL
broad : broadest == wise : ?'wisest'  :: WIN
hard : harder == poor : ?'poorer'  :: WIN
    
